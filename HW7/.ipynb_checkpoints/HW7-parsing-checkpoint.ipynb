{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7: Neural Transition-Based Dependency Parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to build a deep learning model for Neural Networks Transition-Based Dependency Parsing. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this exercise, you will need to complete the code and build a deep learning model for dependency parsing. We will evaluate the model on the subset of Penn Treebank (annotated with Universal Dependencies). \n",
    "\n",
    "We provide the code for data preparation and the skeleton for PartialParse class. You do not need to understand the code outside of this notebook. \n",
    "\n",
    "This homework and the starter codes are adopt from Stanford University class CS224n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework does not require you to use Google Cloud as the model is quite small (but you can still use it if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to shut down your instance on Gcloud when you are not using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation will be a transition-based parser, which incrementally builds\n",
    "up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows:\n",
    "- A stack of words that are currently being processed.\n",
    "- A buffer of words yet to be processed.\n",
    "- A list of dependencies predicted by the parser.\n",
    "\n",
    "Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words\n",
    "of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is\n",
    "empty and the stack is size 1. The following transitions can be applied:\n",
    "- SHIFT: removes the first word from the buffer and pushes it onto the stack.\n",
    "- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the\n",
    "first item and removes the second item from the stack.\n",
    "- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n",
    "item and removes the first item from the stack.\n",
    "\n",
    "Your parser will decide among transitions at each state using a neural network classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1 (Written):\n",
    "Go through the sequence of transitions needed for parsing the sentence “I parsed\n",
    "this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the\n",
    "configuration of the stack and buffer, as well as what transition was applied this step and what new\n",
    "dependency was added (if any). The first three steps are provided below as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following table (double click the table and fill in the rest):\n",
    "\n",
    "| stack    |  buffer |  new dependency | transition |\n",
    "| :------: |:------: | :-------------: | :--------: |\n",
    "| \\[ROOT\\]            | \\[I, parsed, this, sentence, correctly\\] |          | Initial Configuration |\n",
    "| \\[ROOT, I\\]         | \\[parsed, this, sentence, correctly\\]    |          | SHIFT |\n",
    "| \\[ROOT, I, parsed\\] | \\[this, sentence, correctly\\]            |          | SHIFT |\n",
    "| \\[ROOT, parsed\\]    | \\[this, sentence, correctly\\]            | parsed→I | LEFT-ARC |\n",
    "| \\[ROOT, parsed, this\\]    | \\[ sentence, correctly\\]           |          | SHIFT |\n",
    "| \\[ROOT, parsed, this, sentence\\]    | \\[ correctly\\]           |          | SHIFT |\n",
    "| \\[ROOT, parsed, sentence\\]    | \\[correctly\\]            | sentence→this | LEFT-ARC |\n",
    "| \\[ROOT, parsed\\]    | \\[correctly\\]            | parsed→sentence | RIGHT-ARC |\n",
    "| \\[ROOT, parsed, correctly\\] | \\[\\]            |          | SHIFT |\n",
    "| \\[ROOT, parsed\\]    | \\[\\]            | parsed→correctly | RIGHT-ARC |\n",
    "| \\[ROOT\\]    | \\[\\]            | ROOT→parsed | RIGHT-ARC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2 (Coding):\n",
    "Implement the __\\_\\_init\\_\\___ and __parse_step__ functions in the PartialParse class. Your code must past both of the following tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        Your code should initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                        last element of the list.\n",
    "            self.buffer: The current buffer represented as a list with the first item on the\n",
    "                         buffer as the first item of the list\n",
    "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                    tuples where each tuple is of the form (head, dependent).\n",
    "                    Order for this list doesn't matter.\n",
    "\n",
    "        The root token should be represented with the string \"ROOT\"\n",
    "\n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                      Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence\n",
    "        self.dependencies = []\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                        and right-arc transitions. You can assume the provided transition is a legal\n",
    "                        transition.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        if transition == 'S':\n",
    "            self.stack = self.stack + [self.buffer[0]]\n",
    "            self.buffer = self.buffer[1:]\n",
    "        elif transition == 'LA':\n",
    "            self.dependencies = self.dependencies+[(self.stack[-1],self.stack[-2])]\n",
    "            self.stack = self.stack[:-2]+[self.stack[-1]]\n",
    "        elif transition == 'RA':\n",
    "            self.dependencies = self.dependencies+[(self.stack[-2],self.stack[-1])]\n",
    "            self.stack = self.stack[:-1]\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied\n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this code\n",
    "def test_step(name, transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "\n",
    "    pp.parse_step(transition)\n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    assert stack == ex_stack, \\\n",
    "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
    "    assert buf == ex_buf, \\\n",
    "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "    print(\"{:} test passed!\".format(name))\n",
    "\n",
    "\n",
    "def test_parse_step():\n",
    "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
    "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
    "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
    "\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = tuple(sorted(dependencies))\n",
    "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
    "    assert dependencies == expected,  \\\n",
    "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
    "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
    "        \"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n"
     ]
    }
   ],
   "source": [
    "test_parse_step()\n",
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3 (Coding):\n",
    "Our network will predict which transition should be applied next to a partial parse. We could use it to parse a single sentence by applying predicted transitions until the parse is complete. However, neural networks run much more efficiently when making predictions about batches of data at a time (i.e., predicting the next transition for a many different partial parses simultaneously). We can parse sentences in minibatches with the following algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this algorithm in the minibatch parse function and run the following test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
    "        model: The model that makes parsing decisions. It is assumed to have a function\n",
    "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "               returns a list of transitions predicted for each parse. That is, after calling\n",
    "                   transitions = model.predict(partial_parses)\n",
    "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "        batch_size: The number of PartialParses to include in each minibatch\n",
    "    Returns:\n",
    "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
    "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
    "                      contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    partial_parses = [PartialParse(i) for i in sentences]\n",
    "    dependencies = []\n",
    "    unfinished_parses = [partial_parses[i:i+batch_size] for i in range(0,len(partial_parses),batch_size)]\n",
    "    for minibatch in unfinished_parses:\n",
    "        \n",
    "        enum_minibatch = np.array([[i,j] for i,j in enumerate(minibatch)])\n",
    "        tmp = [[]]*batch_size\n",
    "\n",
    "        while(enum_minibatch.size > 0):\n",
    "            transitions = model.predict(enum_minibatch[:,1])\n",
    "            tmp_enum = enum_minibatch\n",
    "            for i in range(len(enum_minibatch)):\n",
    "                if not (len(enum_minibatch[i][1].stack) == 1 and len(enum_minibatch[i][1].buffer) == 0): \n",
    "                    enum_minibatch[i][1].parse_step(transitions[i]) \n",
    " \n",
    "                if len(enum_minibatch[i][1].stack) == 1 and len(enum_minibatch[i][1].buffer) == 0:  \n",
    "                    tmp[enum_minibatch[i][0]] = enum_minibatch[i][1].dependencies\n",
    "                    tmp_enum = np.concatenate((tmp_enum[:i],tmp_enum[i+1:]),axis=0)\n",
    "                    \n",
    "            enum_minibatch = tmp_enum\n",
    "\n",
    "        dependencies += (tmp)\n",
    "        del tmp\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this code\n",
    "class DummyModel(object):\n",
    "    \"\"\"Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]\n",
    "\n",
    "\n",
    "def test_dependencies(name, deps, ex_deps):\n",
    "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
    "    deps = tuple(sorted(deps))\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
    "\n",
    "\n",
    "def test_minibatch_parse():\n",
    "    \"\"\"Simple tests for the minibatch_parse function\n",
    "    Warning: these are not exhaustive\n",
    "    \"\"\"\n",
    "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "                 [\"left\", \"arcs\", \"only\"],\n",
    "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
    "    test_dependencies(\"minibatch_parse\", deps[0],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[1],\n",
    "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[2],\n",
    "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
    "    test_dependencies(\"minibatch_parse\", deps[3],\n",
    "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
    "    print(\"minibatch_parse test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "test_minibatch_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parser_utils import minibatches, load_and_preprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data. We will use a subset of Penn Treebank and pretrained embeddings in this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next. First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the original neural dependency parsing paper: A Fast and Accurate Dependency Parser using Neural Networks. \n",
    "\n",
    "The function extracting these features has been implemented for you in parser_utils. This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.). They can be represented as a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "took 1.58 seconds\n",
      "Building parser...\n",
      "took 0.03 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 1.64 seconds\n",
      "Vectorizing data...\n",
      "took 0.05 seconds\n",
      "Preprocessing training data...\n",
      "took 0.83 seconds\n"
     ]
    }
   ],
   "source": [
    "parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48390 500 500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples), len(dev_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5157, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the full batch of our subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_gen = minibatches(train_examples, len(train_examples))\n",
    "x_train, y_train = minibatch_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48390, 36)\n",
      "(48390, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5155 5156  621 5155 5155 5155  329   87  149  486  165 5155 5155 5155\n",
      " 5155 5155 5155 5155   83   84   53   83   83   83   47   46   39   44\n",
      "   47   83   83   83   83   83   83   83]\n",
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "## Sample features\n",
    "print(x_train[10])\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Reshape, Dropout, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4 (Coding):\n",
    "Build and train a keras model to predict an action for each state of of the input. This is a simple classification task. \n",
    "- The input and output of the model must match the dimention of x_train and y_train.\n",
    "- The model must use the provided pretrained embeddings\n",
    "- The model could comprise of only a feedforward layer and a dropout\n",
    "- Training loss should be around 0.1 or below, and training categorical_accuracy above 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kok/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kok/anaconda3/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 36, 50)            257850    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36, 100)           5100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 36, 100)           10100     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               360100    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 633,453\n",
      "Trainable params: 633,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, TimeDistributed\n",
    "from keras.metrics import categorical_accuracy\n",
    "def get_your_object():\n",
    "    max_features = embeddings.shape[0]\n",
    "    max_len = x_train.shape[1]\n",
    "    #replace \"pass\" with code for your neural net\n",
    "    input1 = Input(shape=(max_len,))\n",
    "    x = Embedding(max_features, 50,weights=[embeddings], input_length=max_len,trainable=True)(input1)\n",
    "    #x = Conv1D(100, 5, strides = 1, padding='same', activation='relu')(x)\n",
    "    #x = TimeDistributed(Dense(5, activation='relu'))(x)\n",
    "    #x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense((y_train.shape[1]), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[categorical_accuracy])\n",
    "    return model\n",
    "\n",
    "obj_model = get_your_object()\n",
    "obj_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kok/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "48390/48390 [==============================] - 38s 777us/step - loss: 0.3147 - categorical_accuracy: 0.8720\n",
      "Epoch 2/10\n",
      "48390/48390 [==============================] - 34s 711us/step - loss: 0.1894 - categorical_accuracy: 0.9236\n",
      "Epoch 3/10\n",
      "48390/48390 [==============================] - 34s 707us/step - loss: 0.1519 - categorical_accuracy: 0.9399\n",
      "Epoch 4/10\n",
      "48390/48390 [==============================] - 33s 691us/step - loss: 0.1278 - categorical_accuracy: 0.9494\n",
      "Epoch 5/10\n",
      "48390/48390 [==============================] - 36s 742us/step - loss: 0.1102 - categorical_accuracy: 0.9586\n",
      "Epoch 6/10\n",
      "48390/48390 [==============================] - 29s 590us/step - loss: 0.0950 - categorical_accuracy: 0.9635\n",
      "Epoch 7/10\n",
      "48390/48390 [==============================] - 29s 597us/step - loss: 0.0832 - categorical_accuracy: 0.9676\n",
      "Epoch 8/10\n",
      "48390/48390 [==============================] - 32s 670us/step - loss: 0.0724 - categorical_accuracy: 0.9725\n",
      "Epoch 9/10\n",
      "48390/48390 [==============================] - 38s 780us/step - loss: 0.0653 - categorical_accuracy: 0.9759\n",
      "Epoch 10/10\n",
      "48390/48390 [==============================] - 38s 778us/step - loss: 0.0599 - categorical_accuracy: 0.9775\n",
      "CPU times: user 25min 33s, sys: 1min 46s, total: 27min 20s\n",
      "Wall time: 5min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1df361d198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "obj_model.fit(x_train,y_train,batch_size=16,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dependency Parsing, we usually report attachment score of the model for evaluation. There are two possible metrics UAS and LAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5 (Written and Coding):\n",
    "Explain how attachment score is calculated and the difference between unlabeled attachment score (UAS) and labeled attachment score (LAS). Which one should we use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer here__:\n",
    "\n",
    "Attachment score is the percentage of words that have the correct head.\n",
    "\n",
    "UAS studies the structure of a dependency tree and assesses whether the output has the correct\n",
    "head and dependency arcs. In addition to the structure score in UAS, LAS also measures the accuracy\n",
    "of the dependency labels on each arc. A third, but less common metric, is used to judge the percentage\n",
    "of sentences that are completely correct in regards to their LAS score. This score is better used to judge\n",
    "how a dependency parser will affect other NLP tools that make use of the dependency parser output.\n",
    "\n",
    "but this homework will use only UAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the score using appropriate metric on dev_set and test_set. The function for calculating scores are provided in parser.parse and the dataset can be passed in as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1588769474298092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UAS, depen = parser.parse(dev_set, obj_model, minibatch_parse, eval_batch_size=256)\n",
    "UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20168208578637511"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UAS, depen = parser.parse(test_set, obj_model, minibatch_parse, eval_batch_size=256)\n",
    "UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, print one sample sentence (in English) in the test set and its predicted dependencies from the model.\n",
    "You can use __parser.id2tok\\[word_id\\]__ to lookup the word in English.\n",
    "\n",
    "__Draw a picture of this sentence with arrows and upload it to my couseville__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5156, 85, 639, 767, 5154, 98, 105, 841, 391, 87]\n",
      "[84, 41, 42, 48, 49, 39, 40, 42, 42, 46]\n",
      "[-1, 2, 3, 0, 5, 3, 8, 8, 3, 3]\n",
      "[-1, 27, 12, 0, 28, 6, 36, 17, 5, 20]\n",
      "Word = <ROOT>  the  dow  fell  <UNK>  %  on  black  monday  .  \n",
      "PoS = <p>:<ROOT>  <p>:DT  <p>:NNP  <p>:VBD  <p>:CD  <p>:NN  <p>:IN  <p>:NNP  <p>:NNP  <p>:.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 1), (3, 2), (5, 4), (3, 5), (8, 7), (8, 6), (3, 8), (3, 9), (0, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 20\n",
    "print(test_set[i]['word'])\n",
    "print(test_set[i]['pos'])\n",
    "print(test_set[i]['head'])\n",
    "print(test_set[i]['label'])\n",
    "print('Word =',''.join([parser.id2tok[i]+'  ' for i in test_set[i]['word']]))\n",
    "print('PoS =',''.join([parser.id2tok[i]+'  ' for i in test_set[i]['pos']]))\n",
    "depen[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://doc-04-a8-docs.googleusercontent.com/docs/securesc/tqv3ev9luo67j4u7api4ed0bb4fuhn4j/lesukbum738ku5o5e2q17fkcdba7j1i2/1552744800000/01909380670798354122/01909380670798354122/1jLlMR9t2hHlJGe0O2hbQr_aHeol9OidU?e=download\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://doc-04-a8-docs.googleusercontent.com/docs/securesc/tqv3ev9luo67j4u7api4ed0bb4fuhn4j/lesukbum738ku5o5e2q17fkcdba7j1i2/1552744800000/01909380670798354122/01909380670798354122/1jLlMR9t2hHlJGe0O2hbQr_aHeol9OidU?e=download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://scontent.fbkk10-1.fna.fbcdn.net/v/t1.0-9/54520859_2050699871665657_8230014571658608640_o.jpg?_nc_cat=105&_nc_eui2=AeGm9Jq4IyspKkx69bpK9Uof7ya4ExEqc_KM2DIGJ0-GhlZCztchFymnagp3dbCZZp3ZQ-17K26QTRRcxAHro1vM3ZXj2bIfGNdahUH1W_F6bA&_nc_ht=scontent.fbkk10-1.fna&oh=ef457626a2798b1c6c86317af1fcd36c&oe=5D242710\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://scontent.fbkk10-1.fna.fbcdn.net/v/t1.0-9/54520859_2050699871665657_8230014571658608640_o.jpg?_nc_cat=105&_nc_eui2=AeGm9Jq4IyspKkx69bpK9Uof7ya4ExEqc_KM2DIGJ0-GhlZCztchFymnagp3dbCZZp3ZQ-17K26QTRRcxAHro1vM3ZXj2bIfGNdahUH1W_F6bA&_nc_ht=scontent.fbkk10-1.fna&oh=ef457626a2798b1c6c86317af1fcd36c&oe=5D242710\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget to shut down your instance on Gcloud when you are not using it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
