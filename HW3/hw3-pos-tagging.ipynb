{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - Neural POS Tagger\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
    "\n",
    "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
    "- Neural POS Tagging with Viterbi / Marginal CRF\n",
    "\n",
    "Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n",
    "\n",
    "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
    "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
    "We also create a word vector for unknown word by random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data.orchid_corpus import get_sentences\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import keras.preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n",
      "[('5', 'NLBL'), ('<full_stop>', 'PUNC'), ('การ', 'FIXN'), ('ออกแบบ', 'VACT'), ('คลังข้อมูล', 'NCMN'), ('มาตรฐาน', 'NCMN'), ('<space>', 'PUNC'), ('คลังข้อมูล', 'NCMN'), ('มาตรฐาน', 'NCMN'), ('หมายถึง', 'VSTA'), ('<space>', 'PUNC'), ('แฟ้มข้อมูล', 'NCMN'), ('ที่', 'PREL'), ('เก็บ', 'VACT'), ('รวบรวม', 'VACT'), ('ข้อมูล', 'NCMN')]\n"
     ]
    }
   ],
   "source": [
    "unk_emb =np.random.randn(32)\n",
    "train_data = get_sentences('train')\n",
    "test_data = get_sentences('test')\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open('basic_ff_embedding.pt', 'rb')\n",
    "embeddings = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "idx_to_word ={}\n",
    "label_to_idx = {}\n",
    "for sentence in train_data:\n",
    "    for word,pos in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)+1\n",
    "            idx_to_word[word_to_idx[word]] = word\n",
    "        if pos not in label_to_idx:\n",
    "            label_to_idx[pos] = len(label_to_idx)+1\n",
    "word_to_idx['UNK'] = len(word_to_idx)\n",
    "\n",
    "n_classes = len(label_to_idx.keys())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29, 327,   5, 328])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_data[100], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 301 ms, sys: 0 ns, total: 301 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
    "\n",
    "evaluation_report is the same as in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputToLabel(yt,seq_len):\n",
    "    out = []\n",
    "    for i in range(0,len(yt)):\n",
    "        if(i==seq_len):\n",
    "            break\n",
    "        out.append(np.argmax(yt[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "    \n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    accuracy = (all_correct / all_count) * 100\n",
    "            \n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "        \n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking,Flatten,Conv1D,InputLayer\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is this section is separated to two groups\n",
    "\n",
    "- Neural POS Tagger (4.1)\n",
    "- Neural CRF POS Tagger (4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Neural POS Tagger  (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 85s 5ms/step - loss: 1.9036 - categorical_accuracy: 0.5451\n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 87s 5ms/step - loss: 0.4278 - categorical_accuracy: 0.9022\n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 87s 5ms/step - loss: 0.2553 - categorical_accuracy: 0.9352\n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 88s 5ms/step - loss: 0.1998 - categorical_accuracy: 0.9459\n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1722 - categorical_accuracy: 0.9521\n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1564 - categorical_accuracy: 0.9551\n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 85s 5ms/step - loss: 0.1446 - categorical_accuracy: 0.9578\n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 85s 5ms/step - loss: 0.1362 - categorical_accuracy: 0.9600\n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1290 - categorical_accuracy: 0.9618\n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 85s 5ms/step - loss: 0.1229 - categorical_accuracy: 0.9630\n",
      "CPU times: user 33min 40s, sys: 4min 58s, total: 38min 39s\n",
      "Wall time: 14min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f984f2fc7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8092</td>\n",
       "      <td>99.3758</td>\n",
       "      <td>99.5921</td>\n",
       "      <td>3662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.8284</td>\n",
       "      <td>94.4835</td>\n",
       "      <td>94.6557</td>\n",
       "      <td>7793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.0045</td>\n",
       "      <td>96.5007</td>\n",
       "      <td>93.6721</td>\n",
       "      <td>16298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9766</td>\n",
       "      <td>99.3654</td>\n",
       "      <td>99.6701</td>\n",
       "      <td>12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>91.6667</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>94.964</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7817</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.2653</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.6374</td>\n",
       "      <td>97.4026</td>\n",
       "      <td>97.5199</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>67.6647</td>\n",
       "      <td>54.4578</td>\n",
       "      <td>60.3471</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>57.6441</td>\n",
       "      <td>62.5</td>\n",
       "      <td>59.9739</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.7866</td>\n",
       "      <td>42.4315</td>\n",
       "      <td>50.6401</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>98.8372</td>\n",
       "      <td>90.4255</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.4504</td>\n",
       "      <td>98.377</td>\n",
       "      <td>97.4042</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.315</td>\n",
       "      <td>84.8915</td>\n",
       "      <td>87.0471</td>\n",
       "      <td>3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.8437</td>\n",
       "      <td>94.6985</td>\n",
       "      <td>94.2691</td>\n",
       "      <td>5198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.0865</td>\n",
       "      <td>71.836</td>\n",
       "      <td>76.1815</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.334</td>\n",
       "      <td>87.6353</td>\n",
       "      <td>87.9833</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.9534</td>\n",
       "      <td>92.637</td>\n",
       "      <td>94.7461</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.4511</td>\n",
       "      <td>99.3074</td>\n",
       "      <td>98.3705</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.6471</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.9343</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.8405</td>\n",
       "      <td>92.2112</td>\n",
       "      <td>93.5074</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.9196</td>\n",
       "      <td>79.0393</td>\n",
       "      <td>81.874</td>\n",
       "      <td>1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.9261</td>\n",
       "      <td>94.8972</td>\n",
       "      <td>92.3448</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>91.5274</td>\n",
       "      <td>83.8251</td>\n",
       "      <td>87.5071</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>92.381</td>\n",
       "      <td>70.46</td>\n",
       "      <td>79.9451</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>96.2963</td>\n",
       "      <td>88.6364</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>92.9825</td>\n",
       "      <td>80.916</td>\n",
       "      <td>86.5306</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>94.4615</td>\n",
       "      <td>97.1519</td>\n",
       "      <td>95.7878</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>70.7965</td>\n",
       "      <td>78.4314</td>\n",
       "      <td>74.4186</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>68.4211</td>\n",
       "      <td>75.7282</td>\n",
       "      <td>71.8894</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>63.9535</td>\n",
       "      <td>61.7978</td>\n",
       "      <td>62.8571</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>87.5</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>64.8148</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>86.6779</td>\n",
       "      <td>91.4591</td>\n",
       "      <td>89.0043</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.6552</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>88.6957</td>\n",
       "      <td>100</td>\n",
       "      <td>94.0092</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>67.7419</td>\n",
       "      <td>53.8462</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>75.817</td>\n",
       "      <td>82.8571</td>\n",
       "      <td>79.1809</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>75</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>92.8571</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>83.871</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.24</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8092  99.3758  99.5921          3662\n",
       "1                2   94.8284  94.4835  94.6557          7793\n",
       "2                3   91.0045  96.5007  93.6721         16298\n",
       "3                4   99.9766  99.3654  99.6701         12840\n",
       "4                5   91.6667  98.5075   94.964            66\n",
       "5                6   99.7817  87.5479  93.2653           457\n",
       "6                7   97.6374  97.4026  97.5199          2025\n",
       "7                8   67.6647  54.4578  60.3471           226\n",
       "8                9   57.6441     62.5  59.9739           230\n",
       "9               10   62.7866  42.4315  50.6401           356\n",
       "10              11   83.3333  98.8372  90.4255            85\n",
       "11              12   96.4504   98.377  97.4042           788\n",
       "12              13    89.315  84.8915  87.0471          3051\n",
       "13              14   93.8437  94.6985  94.2691          5198\n",
       "14              15   81.0865   71.836  76.1815           806\n",
       "15              16    88.334  87.6353  87.9833          2105\n",
       "16              17   96.9534   92.637  94.7461           541\n",
       "17              18   97.4511  99.3074  98.3705          1147\n",
       "18              19   97.6471  96.2319  96.9343           332\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21   94.8405  92.2112  93.5074          1397\n",
       "21              22   84.9196  79.0393   81.874          1267\n",
       "22              23   89.9261  94.8972  92.3448          1339\n",
       "23              24   91.5274  83.8251  87.5071           767\n",
       "24              25    92.381    70.46  79.9451           291\n",
       "25              26   96.2963  88.6364  92.3077           156\n",
       "26              27   92.9825   80.916  86.5306           106\n",
       "27              29   94.4615  97.1519  95.7878           307\n",
       "28              30   70.7965  78.4314  74.4186            80\n",
       "29              31   68.4211  75.7282  71.8894            78\n",
       "30              32   63.9535  61.7978  62.8571           110\n",
       "31              33      87.5  51.4706  64.8148            35\n",
       "32              34   86.6779  91.4591  89.0043           514\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100    81.25  89.6552            13\n",
       "35              37   88.6957      100  94.0092           102\n",
       "36              38   67.7419  53.8462       60            21\n",
       "37              39    75.817  82.8571  79.1809           116\n",
       "38              40       100      100      100           280\n",
       "39              41   71.4286       75  73.1707            15\n",
       "40              42   92.8571  76.4706   83.871            13\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.24                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.5 s, sys: 9.87 s, total: 1min 3s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Neural POS Tagger - Fix Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 1\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "(You may want to read about Keras's Masking layer and Trainable parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import emb_reader\n",
    "embeddings = emb_reader.get_embeddings()\n",
    "vector_size = embeddings['การ'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((len(idx_to_word),vector_size))\n",
    "for i in idx_to_word:\n",
    "    if idx_to_word[i] in embeddings:\n",
    "        embedding_matrix[i] = embeddings[idx_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 102, 64)           961216    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 102, 64)           18624     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 982,960\n",
      "Trainable params: 21,744\n",
      "Non-trainable params: 961,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),64,input_length=102,embeddings_initializer=Constant(embedding_matrix),mask_zero=True,trainable=False))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 83s 5ms/step - loss: 2.2330 - categorical_accuracy: 0.4146\n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 90s 5ms/step - loss: 1.3125 - categorical_accuracy: 0.6393\n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 83s 4ms/step - loss: 1.0629 - categorical_accuracy: 0.6946\n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 83s 4ms/step - loss: 0.9366 - categorical_accuracy: 0.7280\n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 83s 4ms/step - loss: 0.8623 - categorical_accuracy: 0.7494\n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 89s 5ms/step - loss: 0.8124 - categorical_accuracy: 0.7622\n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 88s 5ms/step - loss: 0.7772 - categorical_accuracy: 0.7719\n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.7506 - categorical_accuracy: 0.7785\n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 89s 5ms/step - loss: 0.7277 - categorical_accuracy: 0.7851\n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 89s 5ms/step - loss: 0.7116 - categorical_accuracy: 0.7888\n",
      "CPU times: user 36min 49s, sys: 5min 40s, total: 42min 29s\n",
      "Wall time: 14min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89591312b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.7825</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.6876</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>78.8364</td>\n",
       "      <td>78.8555</td>\n",
       "      <td>78.8459</td>\n",
       "      <td>6504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>75.0896</td>\n",
       "      <td>76.9436</td>\n",
       "      <td>76.0053</td>\n",
       "      <td>12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70.7736</td>\n",
       "      <td>87.9276</td>\n",
       "      <td>78.4235</td>\n",
       "      <td>11362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.082</td>\n",
       "      <td>86.5672</td>\n",
       "      <td>90.625</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>79.2079</td>\n",
       "      <td>61.3027</td>\n",
       "      <td>69.1145</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.7437</td>\n",
       "      <td>94.5166</td>\n",
       "      <td>96.5839</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>80</td>\n",
       "      <td>3.85542</td>\n",
       "      <td>7.35632</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>60.3175</td>\n",
       "      <td>41.3043</td>\n",
       "      <td>49.0323</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>56.6986</td>\n",
       "      <td>28.2479</td>\n",
       "      <td>37.7088</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>88.4615</td>\n",
       "      <td>26.7442</td>\n",
       "      <td>41.0714</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>92.7591</td>\n",
       "      <td>92.7591</td>\n",
       "      <td>92.7591</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>80.8533</td>\n",
       "      <td>63.8008</td>\n",
       "      <td>71.3219</td>\n",
       "      <td>2293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>92.443</td>\n",
       "      <td>90.9273</td>\n",
       "      <td>91.6789</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>63.2653</td>\n",
       "      <td>49.7326</td>\n",
       "      <td>55.6886</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>77.8727</td>\n",
       "      <td>83.5137</td>\n",
       "      <td>80.5946</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>93.1481</td>\n",
       "      <td>86.1301</td>\n",
       "      <td>89.5018</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>98.6523</td>\n",
       "      <td>95.0649</td>\n",
       "      <td>96.8254</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>96.1905</td>\n",
       "      <td>58.5507</td>\n",
       "      <td>72.7928</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>92.1811</td>\n",
       "      <td>75.9322</td>\n",
       "      <td>83.2714</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.7116</td>\n",
       "      <td>90.495</td>\n",
       "      <td>92.0752</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>63.7069</td>\n",
       "      <td>46.1011</td>\n",
       "      <td>53.4926</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>83.1652</td>\n",
       "      <td>93.4798</td>\n",
       "      <td>88.0214</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>89.3885</td>\n",
       "      <td>54.3169</td>\n",
       "      <td>67.5731</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>83.1776</td>\n",
       "      <td>43.0993</td>\n",
       "      <td>56.7783</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>88.3117</td>\n",
       "      <td>77.2727</td>\n",
       "      <td>82.4242</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>24.4275</td>\n",
       "      <td>39.2638</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.9929</td>\n",
       "      <td>84.1772</td>\n",
       "      <td>88.8147</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>86.8421</td>\n",
       "      <td>32.3529</td>\n",
       "      <td>47.1429</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>53.0303</td>\n",
       "      <td>33.9806</td>\n",
       "      <td>41.4201</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>88.8889</td>\n",
       "      <td>22.4719</td>\n",
       "      <td>35.8744</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>56.7901</td>\n",
       "      <td>90.0356</td>\n",
       "      <td>69.649</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>85.4167</td>\n",
       "      <td>80.3922</td>\n",
       "      <td>82.8283</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>72.0721</td>\n",
       "      <td>57.1429</td>\n",
       "      <td>63.745</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>92.8571</td>\n",
       "      <td>96.2963</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>47.0588</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=79.27</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.7825  99.5929  99.6876          3670\n",
       "1                2   78.8364  78.8555  78.8459          6504\n",
       "2                3   75.0896  76.9436  76.0053         12995\n",
       "3                4   70.7736  87.9276  78.4235         11362\n",
       "4                5    95.082  86.5672   90.625            58\n",
       "5                6   79.2079  61.3027  69.1145           320\n",
       "6                7   98.7437  94.5166  96.5839          1965\n",
       "7                8        80  3.85542  7.35632            16\n",
       "8                9   60.3175  41.3043  49.0323           152\n",
       "9               10   56.6986  28.2479  37.7088           237\n",
       "10              11   88.4615  26.7442  41.0714            23\n",
       "11              12   92.7591  92.7591  92.7591           743\n",
       "12              13   80.8533  63.8008  71.3219          2293\n",
       "13              14    92.443  90.9273  91.6789          4991\n",
       "14              15   63.2653  49.7326  55.6886           558\n",
       "15              16   77.8727  83.5137  80.5946          2006\n",
       "16              17   93.1481  86.1301  89.5018           503\n",
       "17              18   98.6523  95.0649  96.8254          1098\n",
       "18              19   96.1905  58.5507  72.7928           202\n",
       "19              20   92.1811  75.9322  83.2714           224\n",
       "20              21   93.7116   90.495  92.0752          1371\n",
       "21              22   63.7069  46.1011  53.4926           739\n",
       "22              23   83.1652  93.4798  88.0214          1319\n",
       "23              24   89.3885  54.3169  67.5731           497\n",
       "24              25   83.1776  43.0993  56.7783           178\n",
       "25              26   88.3117  77.2727  82.4242           136\n",
       "26              27       100  24.4275  39.2638            32\n",
       "27              29   93.9929  84.1772  88.8147           266\n",
       "28              30   86.8421  32.3529  47.1429            33\n",
       "29              31   53.0303  33.9806  41.4201            35\n",
       "30              32   88.8889  22.4719  35.8744            40\n",
       "31              33         -        0        -             0\n",
       "32              34   56.7901  90.0356   69.649           506\n",
       "33              35         -        0        -             0\n",
       "34              36       100      100      100            16\n",
       "35              37   85.4167  80.3922  82.8283            82\n",
       "36              38         -        0        -             0\n",
       "37              39   72.0721  57.1429   63.745            80\n",
       "38              40       100  92.8571  96.2963           260\n",
       "39              41         -        0        -             0\n",
       "40              42       100  47.0588       64             8\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=79.27                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.9 s, sys: 10.6 s, total: 1min 8s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in emb :\n",
    "        return emb[word]\n",
    "    else :\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 34.5 ms, total: 511 ms\n",
      "Wall time: 508 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.array([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = np.array([sent2labels(sent) for sent in train_data])\n",
    "x_test = np.array([sent2features(sent, embeddings) for sent in test_data])\n",
    "y_test = np.array([sent2labels(sent) for sent in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(i) for i in x_train)\n",
    "x_train = np.array([np.concatenate((i,[np.zeros(vector_size)]*(max_len - len(i))),axis = 0) if len(i)<max_len else i for i in x_train])\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=max_len, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test = np.array([np.concatenate((i,[np.zeros(vector_size)]*(max_len - len(i))),axis = 0) if len(i)<max_len else i for i in x_test])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "#x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=103, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.041246   -0.08056918 -0.21603911  0.03531642  0.01896307  0.10224628\n",
      " -0.14995357  0.12535487  0.02024684  0.22443148 -0.29895535 -0.19694647\n",
      " -0.1635168   0.08557106 -0.17634703  0.01820213 -0.00468827 -0.07651532\n",
      " -0.05876088  0.15585257 -0.02346553 -0.11359906 -0.00310849  0.03356488\n",
      "  0.14015509 -0.09045982  0.01143226  0.00039972  0.07332941  0.08260775\n",
      " -0.11846358  0.02441154  0.00845897  0.27604362 -0.04589748  0.00915465\n",
      "  0.07176109  0.21123503  0.00435497  0.13480981  0.04913695  0.05938303\n",
      " -0.08741292  0.22676456 -0.03131349 -0.05550113 -0.088519    0.0824531\n",
      "  0.04506927 -0.00963591 -0.1833221   0.0277024   0.03430984 -0.02346132\n",
      " -0.08337204  0.05305323 -0.03119821  0.03707563 -0.10878314  0.01872645\n",
      "  0.13266806  0.00048555  0.16408308 -0.21319303] (18500, 102, 64)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100][3],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('รัฐมนตรีว่าการ', 'NCMN'), ('กระทรวงวิทยาศาสตร์เทคโนโลยีและการพลังงาน', 'NPRP'), ('<space>', 'PUNC'), ('ประธานกรรมการ', 'NCMN')]\n",
      "[ 0.041246   -0.08056918 -0.2160391   0.03531642  0.01896307  0.10224628\n",
      " -0.14995357  0.12535487  0.02024684  0.22443148 -0.29895535 -0.19694647\n",
      " -0.1635168   0.08557106 -0.17634703  0.01820213 -0.00468827 -0.07651532\n",
      " -0.05876088  0.15585257 -0.02346553 -0.11359906 -0.00310849  0.03356488\n",
      "  0.14015509 -0.09045982  0.01143226  0.00039972  0.07332941  0.08260775\n",
      " -0.11846358  0.02441154  0.00845897  0.27604362 -0.04589748  0.00915465\n",
      "  0.07176109  0.21123503  0.00435497  0.1348098   0.04913695  0.05938303\n",
      " -0.08741292  0.22676456 -0.03131349 -0.05550113 -0.088519    0.0824531\n",
      "  0.04506927 -0.00963591 -0.1833221   0.0277024   0.03430984 -0.02346132\n",
      " -0.08337204  0.05305323 -0.03119821  0.03707563 -0.10878314  0.01872645\n",
      "  0.13266806  0.00048555  0.16408308 -0.21319303]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[100])\n",
    "print(embeddings['ประธานกรรมการ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_26 (Bidirectio (None, 102, 64)           18624     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 21,744\n",
      "Trainable params: 21,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Dense(64, input_shape=x_train.shape, activation='relu'))\n",
    "model.add(InputLayer(input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "#model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 73s 4ms/step - loss: 0.8688 - categorical_accuracy: 0.8872\n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 73s 4ms/step - loss: 0.2967 - categorical_accuracy: 0.9211\n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 72s 4ms/step - loss: 0.2456 - categorical_accuracy: 0.9346\n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 72s 4ms/step - loss: 0.2153 - categorical_accuracy: 0.9415\n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 77s 4ms/step - loss: 0.1944 - categorical_accuracy: 0.9470\n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 78s 4ms/step - loss: 0.1791 - categorical_accuracy: 0.9508\n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 78s 4ms/step - loss: 0.1682 - categorical_accuracy: 0.9535\n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 75s 4ms/step - loss: 0.1599 - categorical_accuracy: 0.9555\n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 76s 4ms/step - loss: 0.1534 - categorical_accuracy: 0.9570\n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 73s 4ms/step - loss: 0.1481 - categorical_accuracy: 0.9584\n",
      "CPU times: user 26min 6s, sys: 3min 47s, total: 29min 53s\n",
      "Wall time: 12min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa80c52ebe0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>99.6741</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.6335</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79.6765</td>\n",
       "      <td>74.0543</td>\n",
       "      <td>76.7626</td>\n",
       "      <td>6108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>72.8166</td>\n",
       "      <td>72.8166</td>\n",
       "      <td>72.8166</td>\n",
       "      <td>12298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>65.8297</td>\n",
       "      <td>75.0658</td>\n",
       "      <td>70.145</td>\n",
       "      <td>9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>86.5672</td>\n",
       "      <td>92.8</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>88.141</td>\n",
       "      <td>52.682</td>\n",
       "      <td>65.9472</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>98.4947</td>\n",
       "      <td>94.4204</td>\n",
       "      <td>96.4145</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>40.625</td>\n",
       "      <td>17.663</td>\n",
       "      <td>24.6212</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>58.5209</td>\n",
       "      <td>21.6925</td>\n",
       "      <td>31.6522</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>23.2558</td>\n",
       "      <td>37.7358</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>89.1198</td>\n",
       "      <td>91.0112</td>\n",
       "      <td>90.0556</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>78.2533</td>\n",
       "      <td>62.5765</td>\n",
       "      <td>69.5424</td>\n",
       "      <td>2249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>89.5195</td>\n",
       "      <td>89.9435</td>\n",
       "      <td>89.731</td>\n",
       "      <td>4937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>54.105</td>\n",
       "      <td>35.8289</td>\n",
       "      <td>43.1099</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>75.442</td>\n",
       "      <td>81.7236</td>\n",
       "      <td>78.4572</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>92.4949</td>\n",
       "      <td>78.0822</td>\n",
       "      <td>84.6797</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>99.0942</td>\n",
       "      <td>94.7186</td>\n",
       "      <td>96.857</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>99.4012</td>\n",
       "      <td>48.1159</td>\n",
       "      <td>64.8438</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>89.4068</td>\n",
       "      <td>71.5254</td>\n",
       "      <td>79.4727</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>92.2656</td>\n",
       "      <td>88.9769</td>\n",
       "      <td>90.5914</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>54.8193</td>\n",
       "      <td>34.0611</td>\n",
       "      <td>42.0162</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>80.0743</td>\n",
       "      <td>91.708</td>\n",
       "      <td>85.4972</td>\n",
       "      <td>1294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>88.4984</td>\n",
       "      <td>30.2732</td>\n",
       "      <td>45.114</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>79.8611</td>\n",
       "      <td>27.845</td>\n",
       "      <td>41.2926</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>86.0465</td>\n",
       "      <td>63.0682</td>\n",
       "      <td>72.7869</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>16.0305</td>\n",
       "      <td>27.6316</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>90.0735</td>\n",
       "      <td>77.5316</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>23.5294</td>\n",
       "      <td>38.0952</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>9.70874</td>\n",
       "      <td>16.9492</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>28.8889</td>\n",
       "      <td>4.62633</td>\n",
       "      <td>7.97546</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>92.4051</td>\n",
       "      <td>71.5686</td>\n",
       "      <td>80.663</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>64.1791</td>\n",
       "      <td>30.7143</td>\n",
       "      <td>41.5459</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>91.7857</td>\n",
       "      <td>95.7169</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>accuracy=72.73</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                0         0        0        -             0\n",
       "1                1   99.6741  99.5929  99.6335          3670\n",
       "2                2   79.6765  74.0543  76.7626          6108\n",
       "3                3   72.8166  72.8166  72.8166         12298\n",
       "4                4   65.8297  75.0658   70.145          9700\n",
       "5                5       100  86.5672     92.8            58\n",
       "6                6    88.141   52.682  65.9472           275\n",
       "7                7   98.4947  94.4204  96.4145          1963\n",
       "8                8         -        0        -             0\n",
       "9                9    40.625   17.663  24.6212            65\n",
       "10              10   58.5209  21.6925  31.6522           182\n",
       "11              11       100  23.2558  37.7358            20\n",
       "12              12   89.1198  91.0112  90.0556           729\n",
       "13              13   78.2533  62.5765  69.5424          2249\n",
       "14              14   89.5195  89.9435   89.731          4937\n",
       "15              15    54.105  35.8289  43.1099           402\n",
       "16              16    75.442  81.7236  78.4572          1963\n",
       "17              17   92.4949  78.0822  84.6797           456\n",
       "18              18   99.0942  94.7186   96.857          1094\n",
       "19              19   99.4012  48.1159  64.8438           166\n",
       "20              20   89.4068  71.5254  79.4727           211\n",
       "21              21   92.2656  88.9769  90.5914          1348\n",
       "22              22   54.8193  34.0611  42.0162           546\n",
       "23              23   80.0743   91.708  85.4972          1294\n",
       "24              24   88.4984  30.2732   45.114           277\n",
       "25              25   79.8611   27.845  41.2926           115\n",
       "26              26   86.0465  63.0682  72.7869           111\n",
       "27              27       100  16.0305  27.6316            21\n",
       "28              29   90.0735  77.5316  83.3333           245\n",
       "29              30       100  23.5294  38.0952            24\n",
       "30              31   66.6667  9.70874  16.9492            10\n",
       "31              32         -        0        -             0\n",
       "32              33         -        0        -             0\n",
       "33              34   28.8889  4.62633  7.97546            26\n",
       "34              35         -        0        -             0\n",
       "35              36         -        0        -             0\n",
       "36              37   92.4051  71.5686   80.663            73\n",
       "37              38         -        0        -             0\n",
       "38              39   64.1791  30.7143  41.5459            43\n",
       "39              40       100  91.7857  95.7169           257\n",
       "40              41         -        0        -             0\n",
       "41              42         -        0        -             0\n",
       "42              43         -        0        -             0\n",
       "43              45         -        0        -             0\n",
       "44              46         -        0        -             0\n",
       "45  accuracy=72.73                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 s, sys: 7.23 s, total: 45.2 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 2\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 2.33 ms, total: 278 ms\n",
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 102, 64)           961216    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           18624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 982,960\n",
      "Trainable params: 982,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),64,input_length=102,embeddings_initializer=Constant(embedding_matrix),mask_zero=True,trainable=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 88s 5ms/step - loss: 1.5392 - categorical_accuracy: 0.6330\n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 99s 5ms/step - loss: 0.3377 - categorical_accuracy: 0.9175\n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 90s 5ms/step - loss: 0.2166 - categorical_accuracy: 0.9425\n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 88s 5ms/step - loss: 0.1756 - categorical_accuracy: 0.9512\n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 87s 5ms/step - loss: 0.1553 - categorical_accuracy: 0.9556\n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1407 - categorical_accuracy: 0.9591\n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1314 - categorical_accuracy: 0.9613\n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 91s 5ms/step - loss: 0.1230 - categorical_accuracy: 0.9633\n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1164 - categorical_accuracy: 0.9651\n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 86s 5ms/step - loss: 0.1105 - categorical_accuracy: 0.9665\n",
      "CPU times: user 34min 34s, sys: 5min 2s, total: 39min 37s\n",
      "Wall time: 14min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8949bee4e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8098</td>\n",
       "      <td>99.6744</td>\n",
       "      <td>99.742</td>\n",
       "      <td>3673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>95.5415</td>\n",
       "      <td>93.2711</td>\n",
       "      <td>94.3926</td>\n",
       "      <td>7693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.3902</td>\n",
       "      <td>95.4053</td>\n",
       "      <td>93.3546</td>\n",
       "      <td>16113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9611</td>\n",
       "      <td>99.5202</td>\n",
       "      <td>99.7402</td>\n",
       "      <td>12860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>92.9577</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>98.3158</td>\n",
       "      <td>89.4636</td>\n",
       "      <td>93.681</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.7724</td>\n",
       "      <td>97.114</td>\n",
       "      <td>97.4421</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>74.2947</td>\n",
       "      <td>57.1084</td>\n",
       "      <td>64.5777</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>74.4409</td>\n",
       "      <td>63.3152</td>\n",
       "      <td>68.4288</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>60.7383</td>\n",
       "      <td>43.1466</td>\n",
       "      <td>50.453</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>61.8705</td>\n",
       "      <td>100</td>\n",
       "      <td>76.4444</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.0928</td>\n",
       "      <td>98.2522</td>\n",
       "      <td>97.1605</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>88.3091</td>\n",
       "      <td>86.1714</td>\n",
       "      <td>87.2272</td>\n",
       "      <td>3097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.576</td>\n",
       "      <td>95.5365</td>\n",
       "      <td>94.5461</td>\n",
       "      <td>5244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>82.4142</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>76.0442</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.8275</td>\n",
       "      <td>87.0525</td>\n",
       "      <td>87.931</td>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.4026</td>\n",
       "      <td>89.8973</td>\n",
       "      <td>93.4996</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>98.4536</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>98.8357</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.654</td>\n",
       "      <td>96.5217</td>\n",
       "      <td>97.0845</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>94.3894</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.4632</td>\n",
       "      <td>92.3432</td>\n",
       "      <td>93.3912</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>77.1014</td>\n",
       "      <td>82.9694</td>\n",
       "      <td>79.9279</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.0276</td>\n",
       "      <td>96.0312</td>\n",
       "      <td>92.3969</td>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>87.0536</td>\n",
       "      <td>85.2459</td>\n",
       "      <td>86.1403</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>90.3581</td>\n",
       "      <td>79.4189</td>\n",
       "      <td>84.5361</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>94.7977</td>\n",
       "      <td>93.1818</td>\n",
       "      <td>93.9828</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>91.9643</td>\n",
       "      <td>78.626</td>\n",
       "      <td>84.7737</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.3535</td>\n",
       "      <td>97.7848</td>\n",
       "      <td>95.5178</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>75.4902</td>\n",
       "      <td>72.6415</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>60.4317</td>\n",
       "      <td>81.5534</td>\n",
       "      <td>69.4215</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>64.4444</td>\n",
       "      <td>65.1685</td>\n",
       "      <td>64.8045</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>63.6364</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>93.4132</td>\n",
       "      <td>83.274</td>\n",
       "      <td>88.0527</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>93.75</td>\n",
       "      <td>96.7742</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.991</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.8357</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>47.8261</td>\n",
       "      <td>56.4103</td>\n",
       "      <td>51.7647</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>75.7764</td>\n",
       "      <td>87.1429</td>\n",
       "      <td>81.0631</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>84.2105</td>\n",
       "      <td>80</td>\n",
       "      <td>82.0513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>94.1176</td>\n",
       "      <td>96.9697</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8098  99.6744   99.742          3673\n",
       "1                2   95.5415  93.2711  94.3926          7693\n",
       "2                3   91.3902  95.4053  93.3546         16113\n",
       "3                4   99.9611  99.5202  99.7402         12860\n",
       "4                5        88  98.5075  92.9577            66\n",
       "5                6   98.3158  89.4636   93.681           467\n",
       "6                7   97.7724   97.114  97.4421          2019\n",
       "7                8   74.2947  57.1084  64.5777           237\n",
       "8                9   74.4409  63.3152  68.4288           233\n",
       "9               10   60.7383  43.1466   50.453           362\n",
       "10              11   61.8705      100  76.4444            86\n",
       "11              12   96.0928  98.2522  97.1605           787\n",
       "12              13   88.3091  86.1714  87.2272          3097\n",
       "13              14    93.576  95.5365  94.5461          5244\n",
       "14              15   82.4142  70.5882  76.0442           792\n",
       "15              16   88.8275  87.0525   87.931          2091\n",
       "16              17   97.4026  89.8973  93.4996           525\n",
       "17              18   98.4536  99.2208  98.8357          1146\n",
       "18              19    97.654  96.5217  97.0845           333\n",
       "19              20   94.3894  96.9492  95.6522           286\n",
       "20              21   94.4632  92.3432  93.3912          1399\n",
       "21              22   77.1014  82.9694  79.9279          1330\n",
       "22              23   89.0276  96.0312  92.3969          1355\n",
       "23              24   87.0536  85.2459  86.1403           780\n",
       "24              25   90.3581  79.4189  84.5361           328\n",
       "25              26   94.7977  93.1818  93.9828           164\n",
       "26              27   91.9643   78.626  84.7737           103\n",
       "27              29   93.3535  97.7848  95.5178           309\n",
       "28              30        70  75.4902  72.6415            77\n",
       "29              31   60.4317  81.5534  69.4215            84\n",
       "30              32   64.4444  65.1685  64.8045           116\n",
       "31              33   83.3333  51.4706  63.6364            35\n",
       "32              34   93.4132   83.274  88.0527           468\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100    93.75  96.7742            15\n",
       "35              37    90.991  99.0196  94.8357           101\n",
       "36              38   47.8261  56.4103  51.7647            22\n",
       "37              39   75.7764  87.1429  81.0631           122\n",
       "38              40       100      100      100           280\n",
       "39              41   84.2105       80  82.0513            16\n",
       "40              42       100  94.1176  96.9697            16\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.14                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.1 s, sys: 9.71 s, total: 1min 1s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 3\n",
    "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n",
    "\n",
    "(If you use your own weight please state so in the answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>\n",
    "I use get_embeddings in embeddings/emb_reader to get word embedding vector of size 64.\n",
    "from f1, precision and recall the best model result in trainable model.\n",
    "And found that many words cannot map to word embedding vector - default [0.]*64 vector-."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 CRF Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
    "\n",
    "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 4\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Do not forget to save this model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "weight_path_viterbi='./weight_model/model_weight_viterbi.h5'\n",
    "callbacks_list_viterbi = [\n",
    "        ModelCheckpoint(\n",
    "            weight_path_viterbi,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='loss',\n",
    "            mode='min',\n",
    "            verbose=1)]\n",
    "#model_feedforward_nn.fit(x, y, epochs, batch_size, verbose,callbacks=callbacks_list_feedforward_nn,validation_data=(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, (18500, 102), 15019, 48)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes,x_train.shape,len(word_to_idx),n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras_contrib.layers.advanced_activations import PELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 64)           961216    \n",
      "_________________________________________________________________\n",
      "pelu_1 (PELU)                (None, 102, 64)           13056     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 102, 48)           5520      \n",
      "=================================================================\n",
      "Total params: 979,792\n",
      "Trainable params: 979,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx), 64, mask_zero=True,input_length=102))\n",
    "#model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "#model.add(TimeDistributed(Dense(n_classes,activation='relu')))\n",
    "#model.add(Dropout(0.2))\n",
    "# use learn_mode = 'join', test_mode = 'viterbi',\n",
    "# sparse_target = True (label indice output)\n",
    "model.add(PELU())\n",
    "model.add(CRF(n_classes,test_mode = 'viterbi'))\n",
    "# crf_accuracy is default to Viterbi acc if using join-mode (default).\n",
    "# One can add crf.marginal_acc if interested, but may slow down learning\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18500/18500 [==============================] - 26s 1ms/step - loss: 47.1345 - crf_viterbi_accuracy: 0.6150\n",
      "\n",
      "Epoch 00001: loss improved from 47.84250 to 47.13446, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 2/30\n",
      "18500/18500 [==============================] - 27s 1ms/step - loss: 45.0188 - crf_viterbi_accuracy: 0.9127\n",
      "\n",
      "Epoch 00002: loss improved from 47.13446 to 45.01877, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 3/30\n",
      "18500/18500 [==============================] - 27s 1ms/step - loss: 44.6840 - crf_viterbi_accuracy: 0.9328\n",
      "\n",
      "Epoch 00003: loss improved from 45.01877 to 44.68395, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 4/30\n",
      "18500/18500 [==============================] - 27s 1ms/step - loss: 44.5768 - crf_viterbi_accuracy: 0.9384\n",
      "\n",
      "Epoch 00004: loss improved from 44.68395 to 44.57676, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 5/30\n",
      "18500/18500 [==============================] - 28s 2ms/step - loss: 44.5285 - crf_viterbi_accuracy: 0.9424\n",
      "\n",
      "Epoch 00005: loss improved from 44.57676 to 44.52855, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 6/30\n",
      "18500/18500 [==============================] - 32s 2ms/step - loss: 44.5057 - crf_viterbi_accuracy: 0.9445\n",
      "\n",
      "Epoch 00006: loss improved from 44.52855 to 44.50575, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 7/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4937 - crf_viterbi_accuracy: 0.9460\n",
      "\n",
      "Epoch 00007: loss improved from 44.50575 to 44.49368, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 8/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4856 - crf_viterbi_accuracy: 0.9470\n",
      "\n",
      "Epoch 00008: loss improved from 44.49368 to 44.48565, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 9/30\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4801 - crf_viterbi_accuracy: 0.9483\n",
      "\n",
      "Epoch 00009: loss improved from 44.48565 to 44.48011, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 10/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4756 - crf_viterbi_accuracy: 0.9492\n",
      "\n",
      "Epoch 00010: loss improved from 44.48011 to 44.47562, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 11/30\n",
      "18500/18500 [==============================] - 32s 2ms/step - loss: 44.4719 - crf_viterbi_accuracy: 0.9498\n",
      "\n",
      "Epoch 00011: loss improved from 44.47562 to 44.47189, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 12/30\n",
      "18500/18500 [==============================] - 32s 2ms/step - loss: 44.4688 - crf_viterbi_accuracy: 0.9505\n",
      "\n",
      "Epoch 00012: loss improved from 44.47189 to 44.46882, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 13/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4658 - crf_viterbi_accuracy: 0.9517\n",
      "\n",
      "Epoch 00013: loss improved from 44.46882 to 44.46583, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 14/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4633 - crf_viterbi_accuracy: 0.9522\n",
      "\n",
      "Epoch 00014: loss improved from 44.46583 to 44.46331, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 15/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4606 - crf_viterbi_accuracy: 0.9530\n",
      "\n",
      "Epoch 00015: loss improved from 44.46331 to 44.46064, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 16/30\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4584 - crf_viterbi_accuracy: 0.9535\n",
      "\n",
      "Epoch 00016: loss improved from 44.46064 to 44.45839, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 17/30\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: 44.4560 - crf_viterbi_accuracy: 0.9543\n",
      "\n",
      "Epoch 00017: loss improved from 44.45839 to 44.45604, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 18/30\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: 44.4539 - crf_viterbi_accuracy: 0.9554\n",
      "\n",
      "Epoch 00018: loss improved from 44.45604 to 44.45387, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 19/30\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4517 - crf_viterbi_accuracy: 0.9559\n",
      "\n",
      "Epoch 00019: loss improved from 44.45387 to 44.45165, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 20/30\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4497 - crf_viterbi_accuracy: 0.9568\n",
      "\n",
      "Epoch 00020: loss improved from 44.45165 to 44.44973, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 21/30\n",
      "18500/18500 [==============================] - 36s 2ms/step - loss: 44.4477 - crf_viterbi_accuracy: 0.9574\n",
      "\n",
      "Epoch 00021: loss improved from 44.44973 to 44.44770, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 22/30\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4459 - crf_viterbi_accuracy: 0.9579\n",
      "\n",
      "Epoch 00022: loss improved from 44.44770 to 44.44590, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 23/30\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4442 - crf_viterbi_accuracy: 0.9586\n",
      "\n",
      "Epoch 00023: loss improved from 44.44590 to 44.44421, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 24/30\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: 44.4424 - crf_viterbi_accuracy: 0.9591\n",
      "\n",
      "Epoch 00024: loss improved from 44.44421 to 44.44239, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 25/30\n",
      "18500/18500 [==============================] - 37s 2ms/step - loss: 44.4411 - crf_viterbi_accuracy: 0.9598\n",
      "\n",
      "Epoch 00025: loss improved from 44.44239 to 44.44109, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 26/30\n",
      "18500/18500 [==============================] - 36s 2ms/step - loss: 44.4396 - crf_viterbi_accuracy: 0.9601\n",
      "\n",
      "Epoch 00026: loss improved from 44.44109 to 44.43963, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 27/30\n",
      "18500/18500 [==============================] - 36s 2ms/step - loss: 44.4383 - crf_viterbi_accuracy: 0.9607\n",
      "\n",
      "Epoch 00027: loss improved from 44.43963 to 44.43835, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 28/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4370 - crf_viterbi_accuracy: 0.9610\n",
      "\n",
      "Epoch 00028: loss improved from 44.43835 to 44.43704, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 29/30\n",
      "18500/18500 [==============================] - 35s 2ms/step - loss: 44.4360 - crf_viterbi_accuracy: 0.9612\n",
      "\n",
      "Epoch 00029: loss improved from 44.43704 to 44.43600, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 30/30\n",
      "18500/18500 [==============================] - 35s 2ms/step - loss: 44.4349 - crf_viterbi_accuracy: 0.9616\n",
      "\n",
      "Epoch 00030: loss improved from 44.43600 to 44.43491, saving model to ./weight_model/model_weight_viterbi.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2dd0544d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y must be label indices (with shape 1 at dim 3) here,\n",
    "# since `sparse_target=True`\n",
    "model.fit(x_train, y_train,epochs=30,batch_size=128,verbose=1,callbacks=callbacks_list_viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8368</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7147</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.7778</td>\n",
       "      <td>93.0771</td>\n",
       "      <td>93.9197</td>\n",
       "      <td>7677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.8074</td>\n",
       "      <td>96.3586</td>\n",
       "      <td>92.9677</td>\n",
       "      <td>16274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9534</td>\n",
       "      <td>99.5898</td>\n",
       "      <td>99.7713</td>\n",
       "      <td>12869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>97.0588</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>88.1226</td>\n",
       "      <td>93.6864</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.3038</td>\n",
       "      <td>97.2102</td>\n",
       "      <td>97.257</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>61.5702</td>\n",
       "      <td>35.9036</td>\n",
       "      <td>45.3577</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>66.3043</td>\n",
       "      <td>49.7283</td>\n",
       "      <td>56.8323</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.1429</td>\n",
       "      <td>41.4779</td>\n",
       "      <td>49.7498</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>90.5263</td>\n",
       "      <td>100</td>\n",
       "      <td>95.0276</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.7581</td>\n",
       "      <td>96.8789</td>\n",
       "      <td>96.8185</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>88.0795</td>\n",
       "      <td>85.1141</td>\n",
       "      <td>86.5714</td>\n",
       "      <td>3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.4843</td>\n",
       "      <td>93.6236</td>\n",
       "      <td>94.052</td>\n",
       "      <td>5139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>79.2492</td>\n",
       "      <td>67.7362</td>\n",
       "      <td>73.0418</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.407</td>\n",
       "      <td>88.2598</td>\n",
       "      <td>88.3333</td>\n",
       "      <td>2120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.012</td>\n",
       "      <td>83.3904</td>\n",
       "      <td>89.6869</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.613</td>\n",
       "      <td>99.1342</td>\n",
       "      <td>98.3677</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>96.8023</td>\n",
       "      <td>96.5217</td>\n",
       "      <td>96.6618</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.2818</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.6109</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.4272</td>\n",
       "      <td>91.9472</td>\n",
       "      <td>92.6813</td>\n",
       "      <td>1393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>82.4573</td>\n",
       "      <td>78.2907</td>\n",
       "      <td>80.32</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.2623</td>\n",
       "      <td>95.3933</td>\n",
       "      <td>91.6894</td>\n",
       "      <td>1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>77.7426</td>\n",
       "      <td>80.5464</td>\n",
       "      <td>79.1197</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>86.859</td>\n",
       "      <td>65.6174</td>\n",
       "      <td>74.7586</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>84.492</td>\n",
       "      <td>89.7727</td>\n",
       "      <td>87.0523</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>93.1624</td>\n",
       "      <td>83.2061</td>\n",
       "      <td>87.9032</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.2308</td>\n",
       "      <td>95.8861</td>\n",
       "      <td>94.5398</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>75</td>\n",
       "      <td>79.4118</td>\n",
       "      <td>77.1429</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>50.8982</td>\n",
       "      <td>82.5243</td>\n",
       "      <td>62.963</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>78.4722</td>\n",
       "      <td>63.4831</td>\n",
       "      <td>70.1863</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>72</td>\n",
       "      <td>52.9412</td>\n",
       "      <td>61.0169</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>91.0683</td>\n",
       "      <td>92.5267</td>\n",
       "      <td>91.7917</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.1786</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.3925</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>56.25</td>\n",
       "      <td>46.1538</td>\n",
       "      <td>50.7042</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>74.1935</td>\n",
       "      <td>82.1429</td>\n",
       "      <td>77.9661</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>93.75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>11.1111</td>\n",
       "      <td>16.6667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>28.5714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.64</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8368  99.5929  99.7147          3670\n",
       "1                2   94.7778  93.0771  93.9197          7677\n",
       "2                3   89.8074  96.3586  92.9677         16274\n",
       "3                4   99.9534  99.5898  99.7713         12869\n",
       "4                5   95.6522  98.5075  97.0588            66\n",
       "5                6       100  88.1226  93.6864           460\n",
       "6                7   97.3038  97.2102   97.257          2021\n",
       "7                8   61.5702  35.9036  45.3577           149\n",
       "8                9   66.3043  49.7283  56.8323           183\n",
       "9               10   62.1429  41.4779  49.7498           348\n",
       "10              11   90.5263      100  95.0276            86\n",
       "11              12   96.7581  96.8789  96.8185           776\n",
       "12              13   88.0795  85.1141  86.5714          3059\n",
       "13              14   94.4843  93.6236   94.052          5139\n",
       "14              15   79.2492  67.7362  73.0418           760\n",
       "15              16    88.407  88.2598  88.3333          2120\n",
       "16              17    97.012  83.3904  89.6869           487\n",
       "17              18    97.613  99.1342  98.3677          1145\n",
       "18              19   96.8023  96.5217  96.6618           333\n",
       "19              20   98.2818  96.9492  97.6109           286\n",
       "20              21   93.4272  91.9472  92.6813          1393\n",
       "21              22   82.4573  78.2907    80.32          1255\n",
       "22              23   88.2623  95.3933  91.6894          1346\n",
       "23              24   77.7426  80.5464  79.1197           737\n",
       "24              25    86.859  65.6174  74.7586           271\n",
       "25              26    84.492  89.7727  87.0523           158\n",
       "26              27   93.1624  83.2061  87.9032           109\n",
       "27              29   93.2308  95.8861  94.5398           303\n",
       "28              30        75  79.4118  77.1429            81\n",
       "29              31   50.8982  82.5243   62.963            85\n",
       "30              32   78.4722  63.4831  70.1863           113\n",
       "31              33        72  52.9412  61.0169            36\n",
       "32              34   91.0683  92.5267  91.7917           520\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100     87.5  93.3333            14\n",
       "35              37   90.1786  99.0196  94.3925           101\n",
       "36              38     56.25  46.1538  50.7042            18\n",
       "37              39   74.1935  82.1429  77.9661           115\n",
       "38              40       100      100      100           280\n",
       "39              41        80       80       80            16\n",
       "40              42       100  88.2353    93.75            15\n",
       "41              43   33.3333  11.1111  16.6667             1\n",
       "42              45         -        0        -             0\n",
       "43              46        50       20  28.5714             1\n",
       "44  accuracy=92.64                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 3.2 s, total: 26.5 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18500/18500 [==============================] - 27s 1ms/step - loss: 44.4312 - crf_viterbi_accuracy: 0.9630\n",
      "\n",
      "Epoch 00001: loss improved from 44.43491 to 44.43117, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 2/100\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: 44.4299 - crf_viterbi_accuracy: 0.9632\n",
      "\n",
      "Epoch 00002: loss improved from 44.43117 to 44.42988, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 3/100\n",
      "18500/18500 [==============================] - 35s 2ms/step - loss: 44.4294 - crf_viterbi_accuracy: 0.9632\n",
      "\n",
      "Epoch 00003: loss improved from 44.42988 to 44.42938, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 4/100\n",
      "18500/18500 [==============================] - 28s 1ms/step - loss: 44.4289 - crf_viterbi_accuracy: 0.9635\n",
      "\n",
      "Epoch 00004: loss improved from 44.42938 to 44.42889, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 5/100\n",
      "18500/18500 [==============================] - 32s 2ms/step - loss: 44.4285 - crf_viterbi_accuracy: 0.9636\n",
      "\n",
      "Epoch 00005: loss improved from 44.42889 to 44.42849, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 6/100\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4281 - crf_viterbi_accuracy: 0.9637\n",
      "\n",
      "Epoch 00006: loss improved from 44.42849 to 44.42811, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 7/100\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4277 - crf_viterbi_accuracy: 0.9640\n",
      "\n",
      "Epoch 00007: loss improved from 44.42811 to 44.42766, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 8/100\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4274 - crf_viterbi_accuracy: 0.9640\n",
      "\n",
      "Epoch 00008: loss improved from 44.42766 to 44.42738, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 9/100\n",
      "18500/18500 [==============================] - 28s 2ms/step - loss: 44.4270 - crf_viterbi_accuracy: 0.9640\n",
      "\n",
      "Epoch 00009: loss improved from 44.42738 to 44.42696, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 10/100\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4268 - crf_viterbi_accuracy: 0.9642\n",
      "\n",
      "Epoch 00010: loss improved from 44.42696 to 44.42678, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 11/100\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: 44.4264 - crf_viterbi_accuracy: 0.9641\n",
      "\n",
      "Epoch 00011: loss improved from 44.42678 to 44.42642, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 12/100\n",
      "18500/18500 [==============================] - 31s 2ms/step - loss: 44.4259 - crf_viterbi_accuracy: 0.9644\n",
      "\n",
      "Epoch 00012: loss improved from 44.42642 to 44.42592, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 13/100\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: 44.4257 - crf_viterbi_accuracy: 0.9644\n",
      "\n",
      "Epoch 00013: loss improved from 44.42592 to 44.42568, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 14/100\n",
      "18500/18500 [==============================] - 28s 2ms/step - loss: 44.4254 - crf_viterbi_accuracy: 0.9644\n",
      "\n",
      "Epoch 00014: loss improved from 44.42568 to 44.42540, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 15/100\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: 44.4252 - crf_viterbi_accuracy: 0.9645\n",
      "\n",
      "Epoch 00015: loss improved from 44.42540 to 44.42519, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 16/100\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4248 - crf_viterbi_accuracy: 0.9646\n",
      "\n",
      "Epoch 00016: loss improved from 44.42519 to 44.42481, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 17/100\n",
      "18500/18500 [==============================] - 27s 1ms/step - loss: 44.4248 - crf_viterbi_accuracy: 0.9645\n",
      "\n",
      "Epoch 00017: loss improved from 44.42481 to 44.42478, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 18/100\n",
      "18500/18500 [==============================] - 26s 1ms/step - loss: 44.4243 - crf_viterbi_accuracy: 0.9647\n",
      "\n",
      "Epoch 00018: loss improved from 44.42478 to 44.42432, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 19/100\n",
      "18500/18500 [==============================] - 26s 1ms/step - loss: 44.4241 - crf_viterbi_accuracy: 0.9648\n",
      "\n",
      "Epoch 00019: loss improved from 44.42432 to 44.42414, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 20/100\n",
      "18500/18500 [==============================] - 28s 2ms/step - loss: 44.4239 - crf_viterbi_accuracy: 0.9649\n",
      "\n",
      "Epoch 00020: loss improved from 44.42414 to 44.42390, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 21/100\n",
      "18500/18500 [==============================] - 30s 2ms/step - loss: 44.4238 - crf_viterbi_accuracy: 0.9648\n",
      "\n",
      "Epoch 00021: loss improved from 44.42390 to 44.42384, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 22/100\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: 44.4234 - crf_viterbi_accuracy: 0.9650\n",
      "\n",
      "Epoch 00022: loss improved from 44.42384 to 44.42345, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 23/100\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: 44.4233 - crf_viterbi_accuracy: 0.9648\n",
      "\n",
      "Epoch 00023: loss improved from 44.42345 to 44.42330, saving model to ./weight_model/model_weight_viterbi.h5\n",
      "Epoch 24/100\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: nan - crf_viterbi_accuracy: 0.4144\n",
      "\n",
      "Epoch 00024: loss did not improve from 44.42330\n",
      "Epoch 25/100\n",
      "18500/18500 [==============================] - 37s 2ms/step - loss: nan - crf_viterbi_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00025: loss did not improve from 44.42330\n",
      "Epoch 26/100\n",
      "18500/18500 [==============================] - 29s 2ms/step - loss: nan - crf_viterbi_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00026: loss did not improve from 44.42330\n",
      "Epoch 27/100\n",
      " 2688/18500 [===>..........................] - ETA: 25s - loss: nan - crf_viterbi_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ccae9b9fc25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list_viterbi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,epochs=100,batch_size=128,verbose=1,callbacks=callbacks_list_viterbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8367</td>\n",
       "      <td>99.5387</td>\n",
       "      <td>99.6875</td>\n",
       "      <td>3668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.0549</td>\n",
       "      <td>93.0286</td>\n",
       "      <td>93.5389</td>\n",
       "      <td>7673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.3577</td>\n",
       "      <td>96.2994</td>\n",
       "      <td>92.6988</td>\n",
       "      <td>16264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9301</td>\n",
       "      <td>99.5434</td>\n",
       "      <td>99.7364</td>\n",
       "      <td>12863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>97.0588</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.3521</td>\n",
       "      <td>88.1226</td>\n",
       "      <td>93.401</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.4001</td>\n",
       "      <td>97.3064</td>\n",
       "      <td>97.3532</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>38.5542</td>\n",
       "      <td>48.855</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>60.4167</td>\n",
       "      <td>47.2826</td>\n",
       "      <td>53.0488</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.4549</td>\n",
       "      <td>41.2396</td>\n",
       "      <td>49.677</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>90.1099</td>\n",
       "      <td>95.3488</td>\n",
       "      <td>92.6554</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.556</td>\n",
       "      <td>98.0025</td>\n",
       "      <td>97.2739</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>87.4314</td>\n",
       "      <td>84.1959</td>\n",
       "      <td>85.7831</td>\n",
       "      <td>3026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.7711</td>\n",
       "      <td>92.7856</td>\n",
       "      <td>93.7678</td>\n",
       "      <td>5093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>75.8483</td>\n",
       "      <td>67.7362</td>\n",
       "      <td>71.5631</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.3117</td>\n",
       "      <td>87.7602</td>\n",
       "      <td>88.0351</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.1591</td>\n",
       "      <td>87.8425</td>\n",
       "      <td>92.2662</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.4533</td>\n",
       "      <td>99.3939</td>\n",
       "      <td>98.4141</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.9228</td>\n",
       "      <td>95.6522</td>\n",
       "      <td>96.7742</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.4446</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.9702</td>\n",
       "      <td>91.5512</td>\n",
       "      <td>92.7449</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>82.2742</td>\n",
       "      <td>76.7311</td>\n",
       "      <td>79.4061</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.6184</td>\n",
       "      <td>95.4642</td>\n",
       "      <td>91.914</td>\n",
       "      <td>1347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>79.0598</td>\n",
       "      <td>80.8743</td>\n",
       "      <td>79.9568</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>86.8167</td>\n",
       "      <td>65.3753</td>\n",
       "      <td>74.5856</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>89.3491</td>\n",
       "      <td>85.7955</td>\n",
       "      <td>87.5362</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>94.0171</td>\n",
       "      <td>83.9695</td>\n",
       "      <td>88.7097</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.4242</td>\n",
       "      <td>96.519</td>\n",
       "      <td>94.4272</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>77</td>\n",
       "      <td>75.4902</td>\n",
       "      <td>76.2376</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>58.156</td>\n",
       "      <td>79.6117</td>\n",
       "      <td>67.2131</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>75</td>\n",
       "      <td>55.618</td>\n",
       "      <td>63.871</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>73.4694</td>\n",
       "      <td>52.9412</td>\n",
       "      <td>61.5385</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>90.9091</td>\n",
       "      <td>92.5267</td>\n",
       "      <td>91.7108</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>68.75</td>\n",
       "      <td>81.4815</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.0901</td>\n",
       "      <td>98.0392</td>\n",
       "      <td>93.8967</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>56.25</td>\n",
       "      <td>46.1538</td>\n",
       "      <td>50.7042</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>77.7027</td>\n",
       "      <td>82.1429</td>\n",
       "      <td>79.8611</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>84.2105</td>\n",
       "      <td>80</td>\n",
       "      <td>82.0513</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>93.75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>14.2857</td>\n",
       "      <td>11.1111</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>28.5714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.44</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8367  99.5387  99.6875          3668\n",
       "1                2   94.0549  93.0286  93.5389          7673\n",
       "2                3   89.3577  96.2994  92.6988         16264\n",
       "3                4   99.9301  99.5434  99.7364         12863\n",
       "4                5   95.6522  98.5075  97.0588            66\n",
       "5                6   99.3521  88.1226   93.401           460\n",
       "6                7   97.4001  97.3064  97.3532          2023\n",
       "7                8   66.6667  38.5542   48.855           160\n",
       "8                9   60.4167  47.2826  53.0488           174\n",
       "9               10   62.4549  41.2396   49.677           346\n",
       "10              11   90.1099  95.3488  92.6554            82\n",
       "11              12    96.556  98.0025  97.2739           785\n",
       "12              13   87.4314  84.1959  85.7831          3026\n",
       "13              14   94.7711  92.7856  93.7678          5093\n",
       "14              15   75.8483  67.7362  71.5631           760\n",
       "15              16   88.3117  87.7602  88.0351          2108\n",
       "16              17   97.1591  87.8425  92.2662           513\n",
       "17              18   97.4533  99.3939  98.4141          1148\n",
       "18              19   97.9228  95.6522  96.7742           330\n",
       "19              20   97.9452  96.9492  97.4446           286\n",
       "20              21   93.9702  91.5512  92.7449          1387\n",
       "21              22   82.2742  76.7311  79.4061          1230\n",
       "22              23   88.6184  95.4642   91.914          1347\n",
       "23              24   79.0598  80.8743  79.9568           740\n",
       "24              25   86.8167  65.3753  74.5856           270\n",
       "25              26   89.3491  85.7955  87.5362           151\n",
       "26              27   94.0171  83.9695  88.7097           110\n",
       "27              29   92.4242   96.519  94.4272           305\n",
       "28              30        77  75.4902  76.2376            77\n",
       "29              31    58.156  79.6117  67.2131            82\n",
       "30              32        75   55.618   63.871            99\n",
       "31              33   73.4694  52.9412  61.5385            36\n",
       "32              34   90.9091  92.5267  91.7108           520\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100    68.75  81.4815            11\n",
       "35              37   90.0901  98.0392  93.8967           100\n",
       "36              38     56.25  46.1538  50.7042            18\n",
       "37              39   77.7027  82.1429  79.8611           115\n",
       "38              40       100      100      100           280\n",
       "39              41   84.2105       80  82.0513            16\n",
       "40              42       100  88.2353    93.75            15\n",
       "41              43   14.2857  11.1111     12.5             1\n",
       "42              45         -        0        -             0\n",
       "43              46        50       20  28.5714             1\n",
       "44  accuracy=92.44                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 3.2 s, total: 25.6 s\n",
      "Wall time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "model.load_weights('./weight_model/model_weight_viterbi.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 CRF Marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 5\n",
    "\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train a new model</b>  but use the pretrained weight from #TODO 4.\n",
    "\n",
    "To finish this excercise you must use the weights from the model trained in previous step and show the evaluation report of marginal problability decoding (testing mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_marginal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 102, 64)           961216    \n",
      "_________________________________________________________________\n",
      "pelu_3 (PELU)                (None, 102, 64)           13056     \n",
      "_________________________________________________________________\n",
      "crf_3 (CRF)                  (None, 102, 48)           5520      \n",
      "=================================================================\n",
      "Total params: 979,792\n",
      "Trainable params: 979,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx), 64, mask_zero=True,input_length=102))\n",
    "model.add(PELU())\n",
    "model.add(CRF(n_classes,test_mode = 'marginal'))\n",
    "model.summary()\n",
    "model.compile('adam', loss=crf_loss, metrics=[crf_marginal_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weight_model/model_weight_viterbi.h5')\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "weight_path_viterbi2='./weight_model/model_weight_viterbi2.h5'\n",
    "callbacks_list_viterbi2 = [\n",
    "        ModelCheckpoint(\n",
    "            weight_path_viterbi2,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='loss',\n",
    "            mode='min',\n",
    "            verbose=1)]\n",
    "#model_feedforward_nn.fit(x, y, epochs, batch_size, verbose,callbacks=callbacks_list_feedforward_nn,validation_data=(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: 44.4244 - crf_marginal_accuracy: 0.8934\n",
      "\n",
      "Epoch 00001: loss improved from inf to 44.42443, saving model to ./weight_model/model_weight_viterbi2.h5\n",
      "Epoch 2/30\n",
      "18500/18500 [==============================] - 34s 2ms/step - loss: 44.4231 - crf_marginal_accuracy: 0.8906\n",
      "\n",
      "Epoch 00002: loss improved from 44.42443 to 44.42305, saving model to ./weight_model/model_weight_viterbi2.h5\n",
      "Epoch 3/30\n",
      "18500/18500 [==============================] - 35s 2ms/step - loss: 44.4227 - crf_marginal_accuracy: 0.8895\n",
      "\n",
      "Epoch 00003: loss improved from 44.42305 to 44.42270, saving model to ./weight_model/model_weight_viterbi2.h5\n",
      "Epoch 4/30\n",
      "18500/18500 [==============================] - 33s 2ms/step - loss: nan - crf_marginal_accuracy: 0.6152\n",
      "\n",
      "Epoch 00004: loss did not improve from 44.42270\n",
      "Epoch 5/30\n",
      "18500/18500 [==============================] - 32s 2ms/step - loss: nan - crf_marginal_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: loss did not improve from 44.42270\n",
      "Epoch 6/30\n",
      " 7296/18500 [==========>...................] - ETA: 21s - loss: nan - crf_marginal_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0a46c137237f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# y must be label indices (with shape 1 at dim 3) here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# since `sparse_target=True`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list_viterbi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y must be label indices (with shape 1 at dim 3) here,\n",
    "# since `sparse_target=True`\n",
    "model.fit(x_train, y_train,epochs=30,batch_size=128,verbose=1,callbacks=callbacks_list_viterbi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.7014</td>\n",
       "      <td>99.6744</td>\n",
       "      <td>99.6879</td>\n",
       "      <td>3673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>85.6002</td>\n",
       "      <td>89.6581</td>\n",
       "      <td>87.5822</td>\n",
       "      <td>7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>87.3645</td>\n",
       "      <td>86.8317</td>\n",
       "      <td>87.0973</td>\n",
       "      <td>14665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.5512</td>\n",
       "      <td>99.7596</td>\n",
       "      <td>12864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>86.8421</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>89.1473</td>\n",
       "      <td>88.1226</td>\n",
       "      <td>88.632</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>96.8584</td>\n",
       "      <td>96.3925</td>\n",
       "      <td>96.6249</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>38.7295</td>\n",
       "      <td>45.5422</td>\n",
       "      <td>41.8605</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>6.21242</td>\n",
       "      <td>25.2717</td>\n",
       "      <td>9.97319</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>60.644</td>\n",
       "      <td>40.4052</td>\n",
       "      <td>48.4979</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>86</td>\n",
       "      <td>100</td>\n",
       "      <td>92.4731</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>68.5783</td>\n",
       "      <td>71.6604</td>\n",
       "      <td>70.0855</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>82.4356</td>\n",
       "      <td>78.3528</td>\n",
       "      <td>80.3424</td>\n",
       "      <td>2816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.2802</td>\n",
       "      <td>85.225</td>\n",
       "      <td>89.0708</td>\n",
       "      <td>4678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>40.4367</td>\n",
       "      <td>41.2656</td>\n",
       "      <td>40.8469</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>84.826</td>\n",
       "      <td>76.1032</td>\n",
       "      <td>80.2282</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>95.6767</td>\n",
       "      <td>87.1575</td>\n",
       "      <td>91.2186</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>95.9494</td>\n",
       "      <td>98.4416</td>\n",
       "      <td>97.1795</td>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>95.9654</td>\n",
       "      <td>96.5217</td>\n",
       "      <td>96.2428</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>97.9381</td>\n",
       "      <td>96.6102</td>\n",
       "      <td>97.2696</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>89.279</td>\n",
       "      <td>93.9934</td>\n",
       "      <td>91.5756</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>60.1368</td>\n",
       "      <td>60.3244</td>\n",
       "      <td>60.2305</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>59.8063</td>\n",
       "      <td>52.5159</td>\n",
       "      <td>55.9245</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>80.1818</td>\n",
       "      <td>48.1967</td>\n",
       "      <td>60.2048</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>80.1775</td>\n",
       "      <td>65.6174</td>\n",
       "      <td>72.1704</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>44.9664</td>\n",
       "      <td>38.0682</td>\n",
       "      <td>41.2308</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>95.0495</td>\n",
       "      <td>73.2824</td>\n",
       "      <td>82.7586</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.3333</td>\n",
       "      <td>87.6582</td>\n",
       "      <td>89.9351</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>41.8079</td>\n",
       "      <td>72.549</td>\n",
       "      <td>53.0466</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>58.8785</td>\n",
       "      <td>61.165</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>28.1955</td>\n",
       "      <td>42.1348</td>\n",
       "      <td>33.7838</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>73.4694</td>\n",
       "      <td>52.9412</td>\n",
       "      <td>61.5385</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>79.2998</td>\n",
       "      <td>92.7046</td>\n",
       "      <td>85.4799</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>89.4231</td>\n",
       "      <td>91.1765</td>\n",
       "      <td>90.2913</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>44.186</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>46.3415</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>66.0377</td>\n",
       "      <td>75</td>\n",
       "      <td>70.2341</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>68.1818</td>\n",
       "      <td>75</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>58.5366</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>26.6667</td>\n",
       "      <td>44.4444</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>14.2857</td>\n",
       "      <td>20</td>\n",
       "      <td>16.6667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>accuracy=85.75</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.7014  99.6744  99.6879          3673\n",
       "1                2   85.6002  89.6581  87.5822          7395\n",
       "2                3   87.3645  86.8317  87.0973         14665\n",
       "3                4   99.9689  99.5512  99.7596         12864\n",
       "4                5   86.8421  98.5075  92.3077            66\n",
       "5                6   89.1473  88.1226   88.632           460\n",
       "6                7   96.8584  96.3925  96.6249          2004\n",
       "7                8   38.7295  45.5422  41.8605           189\n",
       "8                9   6.21242  25.2717  9.97319            93\n",
       "9               10    60.644  40.4052  48.4979           339\n",
       "10              11        86      100  92.4731            86\n",
       "11              12   68.5783  71.6604  70.0855           574\n",
       "12              13   82.4356  78.3528  80.3424          2816\n",
       "13              14   93.2802   85.225  89.0708          4678\n",
       "14              15   40.4367  41.2656  40.8469           463\n",
       "15              16    84.826  76.1032  80.2282          1828\n",
       "16              17   95.6767  87.1575  91.2186           509\n",
       "17              18   95.9494  98.4416  97.1795          1137\n",
       "18              19   95.9654  96.5217  96.2428           333\n",
       "19              20   97.9381  96.6102  97.2696           285\n",
       "20              21    89.279  93.9934  91.5756          1424\n",
       "21              22   60.1368  60.3244  60.2305           967\n",
       "22              23   59.8063  52.5159  55.9245           741\n",
       "23              24   80.1818  48.1967  60.2048           441\n",
       "24              25   80.1775  65.6174  72.1704           271\n",
       "25              26   44.9664  38.0682  41.2308            67\n",
       "26              27   95.0495  73.2824  82.7586            96\n",
       "27              29   92.3333  87.6582  89.9351           277\n",
       "28              30   41.8079   72.549  53.0466            74\n",
       "29              31   58.8785   61.165       60            63\n",
       "30              32   28.1955  42.1348  33.7838            75\n",
       "31              33   73.4694  52.9412  61.5385            36\n",
       "32              34   79.2998  92.7046  85.4799           521\n",
       "33              35   55.5556  55.5556  55.5556             5\n",
       "34              36       100      100      100            16\n",
       "35              37   89.4231  91.1765  90.2913            93\n",
       "36              38    44.186  48.7179  46.3415            19\n",
       "37              39   66.0377       75  70.2341           105\n",
       "38              40       100      100      100           280\n",
       "39              41   68.1818       75  71.4286            15\n",
       "40              42        50  70.5882  58.5366            12\n",
       "41              43   26.6667  44.4444  33.3333             4\n",
       "42              44         0        0        -             0\n",
       "43              45         0        0        -             0\n",
       "44              46   14.2857       20  16.6667             1\n",
       "45              47         0        0        -             0\n",
       "46  accuracy=85.75                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 5.68 s, total: 39.9 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.save_weights('/data/my_pos_no_crf.h5')\n",
    "#model.load_weights('/data/my_pos_no_crf.h5')\n",
    "model.load_weights('./weight_model/model_weight_viterbi2.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 6\n",
    "\n",
    "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (Which model perform better, why? / Which model should be faster? Is it true in this case, why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>\n",
    "\n",
    "For prediction (test phrase), one can choose either Viterbi best path (class indices) or marginal probabilities if probabilities are needed. However, if one chooses *join mode* for training, Viterbi output is typically better than marginal output, but the marginal output will still perform reasonably close, while if *marginal mode* is used for training, marginal output usually performs much better. \n",
    "For accuracy, Viterbi better than Marginal because of class indices.\n",
    "But Marginal may faster due to probabilities. In this case the time of training for each epoch - both Viterbi and Marginal - is about 33s no different on 128-batch-size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
