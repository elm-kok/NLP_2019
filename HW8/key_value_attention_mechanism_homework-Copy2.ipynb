{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
    "\n",
    "In this homework, you will create an MT model with key-value attention mechnism that coverts names of constituency MP candidates in the 2019 Thai general election from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "We have generated a toy dataset using names of constituency MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('mp_name_th_en.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    name_th = []\n",
    "    name_en = []\n",
    "    for row in readCSV:\n",
    "        name_th.append(row[0])\n",
    "        name_en.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไกรสีห์ kraisi\n",
      "พัชรี phatri\n",
      "ธีระ thira\n",
      "วุฒิกร wutthikon\n",
      "ไสว sawai\n",
      "สัมภาษณ์  samphat\n",
      "วศิน wasin\n",
      "ทินวัฒน์ thinwat\n",
      "ศักดินัย sakdinai\n",
      "สุรศักดิ์ surasak\n"
     ]
    }
   ],
   "source": [
    "for th, en in zip(name_th[:10],name_en[:10]):\n",
    "    print(th,en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Preprocess dataset for Keras (1 point)\n",
    "* 2 dictionaries for indexing (1 for input and another for output)\n",
    "* DON'T FORGET TO INCLUDE special token for padding\n",
    "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n",
    "* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 10887\n",
      "Vocab size: 66\n",
      "Max input lenght: 20 \n",
      "\n",
      "Output vocab size: 25\n",
      "Max output lenght: 20 \n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 4 5 6 5 1] kraisi\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n",
      "{0: '<PAD>', 1: '</S>', 2: 'k', 3: 'r', 4: 'a', 5: 'i', 6: 's', 7: 'p', 8: 'h', 9: 't', 10: 'w', 11: 'u', 12: 'o', 13: 'n', 14: 'm', 15: 'd', 16: 'e', 17: 'c', 18: 'l', 19: 'g', 20: 'y', 21: 'b', 22: 'f', 23: '-'}\n",
      "(10887, 20, 66) (10887, 20, 25)\n"
     ]
    }
   ],
   "source": [
    "#FILL YOUR CODE HERE\n",
    "input_dict = dict()\n",
    "output_dict = dict()\n",
    "input_dict['<PAD>'] = 0\n",
    "output_dict['<PAD>'] = 0\n",
    "#input_dict['</S>'] = 1\n",
    "output_dict['</S>'] = 1\n",
    "\n",
    "rev_input = dict()\n",
    "rev_output = dict()\n",
    "\n",
    "for word in name_th:\n",
    "    for char in word:\n",
    "        if char not in input_dict:\n",
    "            input_dict[char] = len(input_dict)\n",
    "\n",
    "for word in name_en:\n",
    "    for char in word:\n",
    "        if char not in output_dict:\n",
    "            output_dict[char] = len(output_dict)\n",
    "\n",
    "for char in input_dict:\n",
    "    rev_input[input_dict[char]] = char\n",
    "for char in output_dict:\n",
    "    rev_output[output_dict[char]] = char\n",
    "    \n",
    "data_size, vocab_size = len(name_th), len(input_dict)+1 \n",
    "output_vocab_size = len(output_dict)+1\n",
    "input_word2char = []\n",
    "output_word2char = []\n",
    "for word in name_th:\n",
    "    input_word2char.append([w for w in word])\n",
    "for word in name_en:\n",
    "    output_word2char.append([w for w in word]+['</S>'])\n",
    "    \n",
    "max_len = max([len(i) for i in input_word2char])\n",
    "output_max_len = max([len(i) for i in output_word2char])\n",
    "\n",
    "print('Data size:',data_size)\n",
    "print('Vocab size:',vocab_size)\n",
    "print('Max input lenght:',max_len,'\\n')\n",
    "print('Output vocab size:',output_vocab_size)\n",
    "print('Max output lenght:',output_max_len,'\\n')\n",
    "\n",
    "X = []\n",
    "for word in name_th:\n",
    "    tmp = []\n",
    "    for char in word:\n",
    "        tmp.append(input_dict[char])\n",
    "    X.append(tmp)\n",
    "    \n",
    "Y = []\n",
    "for word in name_en:\n",
    "    tmp = []\n",
    "    for char in word:\n",
    "        tmp.append(output_dict[char])\n",
    "    tmp.append(output_dict['</S>'])\n",
    "    Y.append(tmp)\n",
    "X = pad_sequences(X,maxlen=max_len)\n",
    "Y = pad_sequences(Y,maxlen=output_max_len)\n",
    "\n",
    "print(Y[0],name_en[0])\n",
    "\n",
    "X= to_categorical(X,vocab_size)\n",
    "X=X.reshape(data_size,max_len ,vocab_size)\n",
    "\n",
    "Y= to_categorical(Y,output_vocab_size)\n",
    "Y=Y.reshape(data_size,output_max_len ,output_vocab_size)\n",
    "\n",
    "print(Y[0][-8:])\n",
    "print(rev_output)\n",
    "print(X.shape,Y.shape)\n",
    "\n",
    "m=data_size\n",
    "Tx=max_len\n",
    "Ty=output_max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "## Task 2: Code your own (key-value) attention mechnism (1 point)\n",
    "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
    "* Define global variables\n",
    "* fill code for one_step_attention function\n",
    "* Hint: use keras.layers.Lambda \n",
    "* HINT: you will probably need more hidden dimmensions than what you've seen in the demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers import Lambda\n",
    "def softMaxAxis1(x):\n",
    "    return softmax(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are global variables (shared layers)\n",
    "## Fill your code here\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "#Attention function###\n",
    "fattn_1 = Dense(100, activation = \"tanh\")\n",
    "fattn_2 = Dense(1, activation = \"relu\")\n",
    "###\n",
    "activator = Activation(softMaxAxis1, name='attention_scores') \n",
    "dotor = Dot(axes = 1)\n",
    "## you are allowed to use code in the demo as your template.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "\n",
    "    #Fill code here\n",
    "    # Repeat the decoder hidden state to concat with encoder hidden states\n",
    "    #key, con = tf.split(a,2,1)\n",
    "    key = Lambda(lambda x: x[:,:,:128])(a)\n",
    "    con = Lambda(lambda x: x[:,:,128:])(a)\n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([s_prev,key])\n",
    "    # attention function\n",
    "    e = fattn_1(concat)\n",
    "    energies =fattn_2(e)\n",
    "    # calculate attention_scores (softmax)\n",
    "    attention_scores = activator(energies)\n",
    "    #calculate a context vector\n",
    "    context = dotor([attention_scores,con])\n",
    "    return context,attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Create and train your encoder/decoder model here (1 point)\n",
    "* HINT: you will probably need more hidden dimmensions than what you've seen in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL CODE HERE\n",
    "n_h = 128 #hidden dimensions for encoder \n",
    "n_s = 128 #hidden dimensions for decoder\n",
    "decoder_LSTM_cell = LSTM(n_s, return_state = True) #decoder_LSTM_cell\n",
    "output_layer = Dense(output_vocab_size, activation=\"softmax\") #softmax output layer\n",
    "#att_layer = Dense(max_len, activation=None) #None output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT YOUR MODEL HERE\n",
    "def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_h -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    vocab_size -- size of the input vocab\n",
    "    output_vocab_size -- size of the output vocab\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model\n",
    "    X = Input(shape=(Tx, vocab_size))\n",
    "    # Define hidden state and cell state for decoder_LSTM_Cell\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = list()\n",
    "    #Encoder Bi-LSTM\n",
    "    h = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(m, Tx, n_h*2))(X)\n",
    "  \n",
    "    #Iterate for Ty steps (Decoding)\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
    "        context,attention = one_step_attention(h, s)\n",
    "       \n",
    "        # Feed the context vector to the decoder LSTM cell\n",
    "        s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
    "        \n",
    "        # Pass the decoder hidden output to the output layer (softmax)\n",
    "        out = output_layer(s)\n",
    "\n",
    "        # Append an output list with the current output\n",
    "        outputs.append(out)\n",
    "\n",
    "    #Create model instance\n",
    "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20, 66)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 256)      199680      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 20, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 256)      0           repeat_vector_1[0][0]            \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 repeat_vector_1[14][0]           \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 repeat_vector_1[15][0]           \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 repeat_vector_1[16][0]           \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 repeat_vector_1[17][0]           \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 repeat_vector_1[18][0]           \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 repeat_vector_1[19][0]           \n",
      "                                                                 lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 100)      25700       concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "                                                                 concatenate_1[10][0]             \n",
      "                                                                 concatenate_1[11][0]             \n",
      "                                                                 concatenate_1[12][0]             \n",
      "                                                                 concatenate_1[13][0]             \n",
      "                                                                 concatenate_1[14][0]             \n",
      "                                                                 concatenate_1[15][0]             \n",
      "                                                                 concatenate_1[16][0]             \n",
      "                                                                 concatenate_1[17][0]             \n",
      "                                                                 concatenate_1[18][0]             \n",
      "                                                                 concatenate_1[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20, 1)        101         dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "                                                                 dense_1[10][0]                   \n",
      "                                                                 dense_1[11][0]                   \n",
      "                                                                 dense_1[12][0]                   \n",
      "                                                                 dense_1[13][0]                   \n",
      "                                                                 dense_1[14][0]                   \n",
      "                                                                 dense_1[15][0]                   \n",
      "                                                                 dense_1[16][0]                   \n",
      "                                                                 dense_1[17][0]                   \n",
      "                                                                 dense_1[18][0]                   \n",
      "                                                                 dense_1[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_scores (Activation)   (None, 20, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "                                                                 dense_2[10][0]                   \n",
      "                                                                 dense_2[11][0]                   \n",
      "                                                                 dense_2[12][0]                   \n",
      "                                                                 dense_2[13][0]                   \n",
      "                                                                 dense_2[14][0]                   \n",
      "                                                                 dense_2[15][0]                   \n",
      "                                                                 dense_2[16][0]                   \n",
      "                                                                 dense_2[17][0]                   \n",
      "                                                                 dense_2[18][0]                   \n",
      "                                                                 dense_2[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_scores[0][0]           \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 attention_scores[1][0]           \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 attention_scores[2][0]           \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 attention_scores[3][0]           \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 attention_scores[4][0]           \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 attention_scores[5][0]           \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 attention_scores[6][0]           \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 attention_scores[7][0]           \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 attention_scores[8][0]           \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 attention_scores[9][0]           \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 attention_scores[10][0]          \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 attention_scores[11][0]          \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 attention_scores[12][0]          \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 attention_scores[13][0]          \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 attention_scores[14][0]          \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 attention_scores[15][0]          \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 attention_scores[16][0]          \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 attention_scores[17][0]          \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 attention_scores[18][0]          \n",
      "                                                                 lambda_38[0][0]                  \n",
      "                                                                 attention_scores[19][0]          \n",
      "                                                                 lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "                                                                 dot_1[10][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[9][2]                     \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 dot_1[14][0]                     \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 dot_1[15][0]                     \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 dot_1[16][0]                     \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 dot_1[17][0]                     \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 dot_1[18][0]                     \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 dot_1[19][0]                     \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 20, 128)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25)           3225        lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "==================================================================================================\n",
      "Total params: 360,290\n",
      "Trainable params: 360,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr= 0.01, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Y.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10887/10887 [==============================] - 33s 3ms/step - loss: 31.4717 - dense_3_loss: 2.4720 - dense_3_acc: 0.9887 - dense_3_acc_1: 0.9886 - dense_3_acc_2: 0.9879 - dense_3_acc_3: 0.9870 - dense_3_acc_4: 0.9856 - dense_3_acc_5: 0.9813 - dense_3_acc_6: 0.9732 - dense_3_acc_7: 0.9557 - dense_3_acc_8: 0.9240 - dense_3_acc_9: 0.8573 - dense_3_acc_10: 0.7514 - dense_3_acc_11: 0.5974 - dense_3_acc_12: 0.3860 - dense_3_acc_13: 0.2108 - dense_3_acc_14: 0.0969 - dense_3_acc_15: 0.0648 - dense_3_acc_16: 0.0161 - dense_3_acc_17: 0.0749 - dense_3_acc_18: 0.0280 - dense_3_acc_19: 0.1533\n",
      "Epoch 2/40\n",
      "10887/10887 [==============================] - 16s 1ms/step - loss: 21.9795 - dense_3_loss: 0.7425 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9843 - dense_3_acc_7: 0.9664 - dense_3_acc_8: 0.9348 - dense_3_acc_9: 0.8692 - dense_3_acc_10: 0.7653 - dense_3_acc_11: 0.6214 - dense_3_acc_12: 0.4275 - dense_3_acc_13: 0.2728 - dense_3_acc_14: 0.1908 - dense_3_acc_15: 0.2170 - dense_3_acc_16: 0.1035 - dense_3_acc_17: 0.2513 - dense_3_acc_18: 0.0620 - dense_3_acc_19: 0.9603\n",
      "Epoch 3/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 19.6507 - dense_3_loss: 0.2320 - dense_3_acc: 0.9997 - dense_3_acc_1: 0.9996 - dense_3_acc_2: 0.9989 - dense_3_acc_3: 0.9980 - dense_3_acc_4: 0.9966 - dense_3_acc_5: 0.9923 - dense_3_acc_6: 0.9842 - dense_3_acc_7: 0.9667 - dense_3_acc_8: 0.9355 - dense_3_acc_9: 0.8720 - dense_3_acc_10: 0.7690 - dense_3_acc_11: 0.6335 - dense_3_acc_12: 0.4344 - dense_3_acc_13: 0.2821 - dense_3_acc_14: 0.2009 - dense_3_acc_15: 0.2326 - dense_3_acc_16: 0.2290 - dense_3_acc_17: 0.3785 - dense_3_acc_18: 0.2385 - dense_3_acc_19: 0.9765\n",
      "Epoch 4/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 17.5622 - dense_3_loss: 0.1257 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9843 - dense_3_acc_7: 0.9666 - dense_3_acc_8: 0.9363 - dense_3_acc_9: 0.8734 - dense_3_acc_10: 0.7738 - dense_3_acc_11: 0.6364 - dense_3_acc_12: 0.4426 - dense_3_acc_13: 0.2915 - dense_3_acc_14: 0.2212 - dense_3_acc_15: 0.2963 - dense_3_acc_16: 0.3707 - dense_3_acc_17: 0.5035 - dense_3_acc_18: 0.4830 - dense_3_acc_19: 0.9825\n",
      "Epoch 5/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 15.7978 - dense_3_loss: 0.0465 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9843 - dense_3_acc_7: 0.9671 - dense_3_acc_8: 0.9377 - dense_3_acc_9: 0.8747 - dense_3_acc_10: 0.7742 - dense_3_acc_11: 0.6363 - dense_3_acc_12: 0.4539 - dense_3_acc_13: 0.3028 - dense_3_acc_14: 0.2386 - dense_3_acc_15: 0.3727 - dense_3_acc_16: 0.4564 - dense_3_acc_17: 0.6225 - dense_3_acc_18: 0.6627 - dense_3_acc_19: 0.9989\n",
      "Epoch 6/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 14.7384 - dense_3_loss: 0.0348 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9843 - dense_3_acc_7: 0.9677 - dense_3_acc_8: 0.9377 - dense_3_acc_9: 0.8737 - dense_3_acc_10: 0.7742 - dense_3_acc_11: 0.6338 - dense_3_acc_12: 0.4613 - dense_3_acc_13: 0.3139 - dense_3_acc_14: 0.2455 - dense_3_acc_15: 0.4015 - dense_3_acc_16: 0.5147 - dense_3_acc_17: 0.6959 - dense_3_acc_18: 0.7347 - dense_3_acc_19: 0.9984\n",
      "Epoch 7/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 13.8884 - dense_3_loss: 0.0270 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9845 - dense_3_acc_7: 0.9677 - dense_3_acc_8: 0.9373 - dense_3_acc_9: 0.8771 - dense_3_acc_10: 0.7794 - dense_3_acc_11: 0.6368 - dense_3_acc_12: 0.4725 - dense_3_acc_13: 0.3333 - dense_3_acc_14: 0.2648 - dense_3_acc_15: 0.4233 - dense_3_acc_16: 0.5507 - dense_3_acc_17: 0.7421 - dense_3_acc_18: 0.7736 - dense_3_acc_19: 0.9984\n",
      "Epoch 8/40\n",
      "10887/10887 [==============================] - 16s 1ms/step - loss: 13.3624 - dense_3_loss: 0.0604 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9925 - dense_3_acc_6: 0.9847 - dense_3_acc_7: 0.9672 - dense_3_acc_8: 0.9373 - dense_3_acc_9: 0.8781 - dense_3_acc_10: 0.7826 - dense_3_acc_11: 0.6454 - dense_3_acc_12: 0.4927 - dense_3_acc_13: 0.3601 - dense_3_acc_14: 0.2941 - dense_3_acc_15: 0.4517 - dense_3_acc_16: 0.5749 - dense_3_acc_17: 0.7571 - dense_3_acc_18: 0.7917 - dense_3_acc_19: 0.9883\n",
      "Epoch 9/40\n",
      "10887/10887 [==============================] - 14s 1ms/step - loss: 12.9500 - dense_3_loss: 0.0651 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9925 - dense_3_acc_6: 0.9844 - dense_3_acc_7: 0.9671 - dense_3_acc_8: 0.9359 - dense_3_acc_9: 0.8792 - dense_3_acc_10: 0.7852 - dense_3_acc_11: 0.6547 - dense_3_acc_12: 0.5100 - dense_3_acc_13: 0.3909 - dense_3_acc_14: 0.3204 - dense_3_acc_15: 0.4591 - dense_3_acc_16: 0.5947 - dense_3_acc_17: 0.7664 - dense_3_acc_18: 0.8036 - dense_3_acc_19: 0.9853\n",
      "Epoch 10/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 11.8077 - dense_3_loss: 0.0193 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9927 - dense_3_acc_6: 0.9847 - dense_3_acc_7: 0.9683 - dense_3_acc_8: 0.9383 - dense_3_acc_9: 0.8826 - dense_3_acc_10: 0.7915 - dense_3_acc_11: 0.6723 - dense_3_acc_12: 0.5384 - dense_3_acc_13: 0.4245 - dense_3_acc_14: 0.3637 - dense_3_acc_15: 0.5023 - dense_3_acc_16: 0.6541 - dense_3_acc_17: 0.8145 - dense_3_acc_18: 0.8666 - dense_3_acc_19: 0.9996\n",
      "Epoch 11/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 11.2561 - dense_3_loss: 0.0175 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9924 - dense_3_acc_6: 0.9848 - dense_3_acc_7: 0.9687 - dense_3_acc_8: 0.9388 - dense_3_acc_9: 0.8835 - dense_3_acc_10: 0.7978 - dense_3_acc_11: 0.6810 - dense_3_acc_12: 0.5585 - dense_3_acc_13: 0.4456 - dense_3_acc_14: 0.3947 - dense_3_acc_15: 0.5352 - dense_3_acc_16: 0.6759 - dense_3_acc_17: 0.8321 - dense_3_acc_18: 0.8820 - dense_3_acc_19: 0.9995\n",
      "Epoch 12/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 10.6827 - dense_3_loss: 0.0165 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9966 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9852 - dense_3_acc_7: 0.9696 - dense_3_acc_8: 0.9415 - dense_3_acc_9: 0.8862 - dense_3_acc_10: 0.8071 - dense_3_acc_11: 0.6937 - dense_3_acc_12: 0.5834 - dense_3_acc_13: 0.4696 - dense_3_acc_14: 0.4284 - dense_3_acc_15: 0.5644 - dense_3_acc_16: 0.6950 - dense_3_acc_17: 0.8500 - dense_3_acc_18: 0.8939 - dense_3_acc_19: 0.9989\n",
      "Epoch 13/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 10.1683 - dense_3_loss: 0.0140 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9966 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9852 - dense_3_acc_7: 0.9696 - dense_3_acc_8: 0.9420 - dense_3_acc_9: 0.8848 - dense_3_acc_10: 0.8112 - dense_3_acc_11: 0.7019 - dense_3_acc_12: 0.5956 - dense_3_acc_13: 0.5001 - dense_3_acc_14: 0.4598 - dense_3_acc_15: 0.5888 - dense_3_acc_16: 0.7164 - dense_3_acc_17: 0.8651 - dense_3_acc_18: 0.9054 - dense_3_acc_19: 0.9992\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10887/10887 [==============================] - 15s 1ms/step - loss: 9.8048 - dense_3_loss: 0.0144 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9966 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9851 - dense_3_acc_7: 0.9702 - dense_3_acc_8: 0.9421 - dense_3_acc_9: 0.8872 - dense_3_acc_10: 0.8189 - dense_3_acc_11: 0.7064 - dense_3_acc_12: 0.6007 - dense_3_acc_13: 0.5147 - dense_3_acc_14: 0.4844 - dense_3_acc_15: 0.6114 - dense_3_acc_16: 0.7272 - dense_3_acc_17: 0.8718 - dense_3_acc_18: 0.9133 - dense_3_acc_19: 0.9990\n",
      "Epoch 15/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 9.2270 - dense_3_loss: 0.0119 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9981 - dense_3_acc_4: 0.9966 - dense_3_acc_5: 0.9930 - dense_3_acc_6: 0.9855 - dense_3_acc_7: 0.9712 - dense_3_acc_8: 0.9450 - dense_3_acc_9: 0.8928 - dense_3_acc_10: 0.8235 - dense_3_acc_11: 0.7235 - dense_3_acc_12: 0.6298 - dense_3_acc_13: 0.5533 - dense_3_acc_14: 0.5304 - dense_3_acc_15: 0.6369 - dense_3_acc_16: 0.7508 - dense_3_acc_17: 0.8882 - dense_3_acc_18: 0.9235 - dense_3_acc_19: 0.9994\n",
      "Epoch 16/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 8.8260 - dense_3_loss: 0.0109 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9967 - dense_3_acc_5: 0.9929 - dense_3_acc_6: 0.9851 - dense_3_acc_7: 0.9719 - dense_3_acc_8: 0.9462 - dense_3_acc_9: 0.8951 - dense_3_acc_10: 0.8303 - dense_3_acc_11: 0.7349 - dense_3_acc_12: 0.6460 - dense_3_acc_13: 0.5781 - dense_3_acc_14: 0.5602 - dense_3_acc_15: 0.6646 - dense_3_acc_16: 0.7672 - dense_3_acc_17: 0.8953 - dense_3_acc_18: 0.9276 - dense_3_acc_19: 0.9993\n",
      "Epoch 17/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 8.4520 - dense_3_loss: 0.0096 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9968 - dense_3_acc_5: 0.9926 - dense_3_acc_6: 0.9859 - dense_3_acc_7: 0.9722 - dense_3_acc_8: 0.9470 - dense_3_acc_9: 0.8982 - dense_3_acc_10: 0.8321 - dense_3_acc_11: 0.7460 - dense_3_acc_12: 0.6626 - dense_3_acc_13: 0.5970 - dense_3_acc_14: 0.5788 - dense_3_acc_15: 0.6800 - dense_3_acc_16: 0.7815 - dense_3_acc_17: 0.9006 - dense_3_acc_18: 0.9357 - dense_3_acc_19: 0.9997\n",
      "Epoch 18/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 8.0963 - dense_3_loss: 0.0103 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9969 - dense_3_acc_5: 0.9929 - dense_3_acc_6: 0.9860 - dense_3_acc_7: 0.9725 - dense_3_acc_8: 0.9481 - dense_3_acc_9: 0.9010 - dense_3_acc_10: 0.8377 - dense_3_acc_11: 0.7557 - dense_3_acc_12: 0.6774 - dense_3_acc_13: 0.6188 - dense_3_acc_14: 0.6033 - dense_3_acc_15: 0.6985 - dense_3_acc_16: 0.7911 - dense_3_acc_17: 0.9070 - dense_3_acc_18: 0.9385 - dense_3_acc_19: 0.9989\n",
      "Epoch 19/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 7.7905 - dense_3_loss: 0.0093 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9968 - dense_3_acc_5: 0.9931 - dense_3_acc_6: 0.9860 - dense_3_acc_7: 0.9740 - dense_3_acc_8: 0.9510 - dense_3_acc_9: 0.9047 - dense_3_acc_10: 0.8461 - dense_3_acc_11: 0.7650 - dense_3_acc_12: 0.6848 - dense_3_acc_13: 0.6284 - dense_3_acc_14: 0.6174 - dense_3_acc_15: 0.7108 - dense_3_acc_16: 0.8073 - dense_3_acc_17: 0.9149 - dense_3_acc_18: 0.9431 - dense_3_acc_19: 0.9995\n",
      "Epoch 20/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 7.4391 - dense_3_loss: 0.0092 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9933 - dense_3_acc_6: 0.9859 - dense_3_acc_7: 0.9746 - dense_3_acc_8: 0.9539 - dense_3_acc_9: 0.9095 - dense_3_acc_10: 0.8498 - dense_3_acc_11: 0.7725 - dense_3_acc_12: 0.6994 - dense_3_acc_13: 0.6431 - dense_3_acc_14: 0.6460 - dense_3_acc_15: 0.7282 - dense_3_acc_16: 0.8195 - dense_3_acc_17: 0.9197 - dense_3_acc_18: 0.9435 - dense_3_acc_19: 0.9993\n",
      "Epoch 21/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 7.2545 - dense_3_loss: 0.0097 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9933 - dense_3_acc_6: 0.9862 - dense_3_acc_7: 0.9756 - dense_3_acc_8: 0.9539 - dense_3_acc_9: 0.9120 - dense_3_acc_10: 0.8563 - dense_3_acc_11: 0.7789 - dense_3_acc_12: 0.7050 - dense_3_acc_13: 0.6544 - dense_3_acc_14: 0.6536 - dense_3_acc_15: 0.7386 - dense_3_acc_16: 0.8254 - dense_3_acc_17: 0.9206 - dense_3_acc_18: 0.9441 - dense_3_acc_19: 0.9993\n",
      "Epoch 22/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 6.9795 - dense_3_loss: 0.0080 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9936 - dense_3_acc_6: 0.9869 - dense_3_acc_7: 0.9758 - dense_3_acc_8: 0.9558 - dense_3_acc_9: 0.9133 - dense_3_acc_10: 0.8585 - dense_3_acc_11: 0.7892 - dense_3_acc_12: 0.7105 - dense_3_acc_13: 0.6657 - dense_3_acc_14: 0.6637 - dense_3_acc_15: 0.7510 - dense_3_acc_16: 0.8394 - dense_3_acc_17: 0.9271 - dense_3_acc_18: 0.9480 - dense_3_acc_19: 0.9994\n",
      "Epoch 23/40\n",
      "10887/10887 [==============================] - 14s 1ms/step - loss: 6.7787 - dense_3_loss: 0.0087 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9971 - dense_3_acc_5: 0.9938 - dense_3_acc_6: 0.9862 - dense_3_acc_7: 0.9760 - dense_3_acc_8: 0.9568 - dense_3_acc_9: 0.9190 - dense_3_acc_10: 0.8633 - dense_3_acc_11: 0.7940 - dense_3_acc_12: 0.7212 - dense_3_acc_13: 0.6741 - dense_3_acc_14: 0.6781 - dense_3_acc_15: 0.7612 - dense_3_acc_16: 0.8445 - dense_3_acc_17: 0.9298 - dense_3_acc_18: 0.9497 - dense_3_acc_19: 0.9992\n",
      "Epoch 24/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 6.5431 - dense_3_loss: 0.0085 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9970 - dense_3_acc_5: 0.9936 - dense_3_acc_6: 0.9871 - dense_3_acc_7: 0.9768 - dense_3_acc_8: 0.9570 - dense_3_acc_9: 0.9208 - dense_3_acc_10: 0.8671 - dense_3_acc_11: 0.7967 - dense_3_acc_12: 0.7257 - dense_3_acc_13: 0.6851 - dense_3_acc_14: 0.6860 - dense_3_acc_15: 0.7694 - dense_3_acc_16: 0.8534 - dense_3_acc_17: 0.9344 - dense_3_acc_18: 0.9530 - dense_3_acc_19: 0.9993\n",
      "Epoch 25/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 6.3286 - dense_3_loss: 0.0072 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9982 - dense_3_acc_4: 0.9969 - dense_3_acc_5: 0.9936 - dense_3_acc_6: 0.9874 - dense_3_acc_7: 0.9763 - dense_3_acc_8: 0.9583 - dense_3_acc_9: 0.9227 - dense_3_acc_10: 0.8706 - dense_3_acc_11: 0.8029 - dense_3_acc_12: 0.7373 - dense_3_acc_13: 0.6909 - dense_3_acc_14: 0.7017 - dense_3_acc_15: 0.7781 - dense_3_acc_16: 0.8640 - dense_3_acc_17: 0.9401 - dense_3_acc_18: 0.9551 - dense_3_acc_19: 0.9994\n",
      "Epoch 26/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 6.1879 - dense_3_loss: 0.0076 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9969 - dense_3_acc_5: 0.9934 - dense_3_acc_6: 0.9875 - dense_3_acc_7: 0.9776 - dense_3_acc_8: 0.9601 - dense_3_acc_9: 0.9248 - dense_3_acc_10: 0.8722 - dense_3_acc_11: 0.8046 - dense_3_acc_12: 0.7368 - dense_3_acc_13: 0.6962 - dense_3_acc_14: 0.7089 - dense_3_acc_15: 0.7848 - dense_3_acc_16: 0.8680 - dense_3_acc_17: 0.9430 - dense_3_acc_18: 0.9580 - dense_3_acc_19: 0.9993\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10887/10887 [==============================] - 15s 1ms/step - loss: 6.0275 - dense_3_loss: 0.0065 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9970 - dense_3_acc_5: 0.9935 - dense_3_acc_6: 0.9879 - dense_3_acc_7: 0.9785 - dense_3_acc_8: 0.9604 - dense_3_acc_9: 0.9246 - dense_3_acc_10: 0.8743 - dense_3_acc_11: 0.8090 - dense_3_acc_12: 0.7424 - dense_3_acc_13: 0.7011 - dense_3_acc_14: 0.7163 - dense_3_acc_15: 0.7945 - dense_3_acc_16: 0.8753 - dense_3_acc_17: 0.9444 - dense_3_acc_18: 0.9584 - dense_3_acc_19: 0.9996\n",
      "Epoch 28/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.8461 - dense_3_loss: 0.0065 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9937 - dense_3_acc_6: 0.9875 - dense_3_acc_7: 0.9780 - dense_3_acc_8: 0.9612 - dense_3_acc_9: 0.9288 - dense_3_acc_10: 0.8803 - dense_3_acc_11: 0.8177 - dense_3_acc_12: 0.7482 - dense_3_acc_13: 0.7102 - dense_3_acc_14: 0.7266 - dense_3_acc_15: 0.8009 - dense_3_acc_16: 0.8806 - dense_3_acc_17: 0.9459 - dense_3_acc_18: 0.9611 - dense_3_acc_19: 0.9995\n",
      "Epoch 29/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.6644 - dense_3_loss: 0.0067 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9971 - dense_3_acc_5: 0.9938 - dense_3_acc_6: 0.9889 - dense_3_acc_7: 0.9801 - dense_3_acc_8: 0.9633 - dense_3_acc_9: 0.9312 - dense_3_acc_10: 0.8833 - dense_3_acc_11: 0.8194 - dense_3_acc_12: 0.7565 - dense_3_acc_13: 0.7181 - dense_3_acc_14: 0.7338 - dense_3_acc_15: 0.8091 - dense_3_acc_16: 0.8857 - dense_3_acc_17: 0.9515 - dense_3_acc_18: 0.9627 - dense_3_acc_19: 0.9991\n",
      "Epoch 30/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.5443 - dense_3_loss: 0.0054 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9984 - dense_3_acc_4: 0.9970 - dense_3_acc_5: 0.9939 - dense_3_acc_6: 0.9888 - dense_3_acc_7: 0.9806 - dense_3_acc_8: 0.9625 - dense_3_acc_9: 0.9306 - dense_3_acc_10: 0.8849 - dense_3_acc_11: 0.8226 - dense_3_acc_12: 0.7598 - dense_3_acc_13: 0.7257 - dense_3_acc_14: 0.7416 - dense_3_acc_15: 0.8132 - dense_3_acc_16: 0.8900 - dense_3_acc_17: 0.9523 - dense_3_acc_18: 0.9634 - dense_3_acc_19: 0.9995\n",
      "Epoch 31/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.3814 - dense_3_loss: 0.0057 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9970 - dense_3_acc_5: 0.9936 - dense_3_acc_6: 0.9891 - dense_3_acc_7: 0.9814 - dense_3_acc_8: 0.9644 - dense_3_acc_9: 0.9334 - dense_3_acc_10: 0.8895 - dense_3_acc_11: 0.8320 - dense_3_acc_12: 0.7629 - dense_3_acc_13: 0.7389 - dense_3_acc_14: 0.7493 - dense_3_acc_15: 0.8172 - dense_3_acc_16: 0.8942 - dense_3_acc_17: 0.9537 - dense_3_acc_18: 0.9649 - dense_3_acc_19: 0.9997\n",
      "Epoch 32/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.2682 - dense_3_loss: 0.0060 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9969 - dense_3_acc_5: 0.9942 - dense_3_acc_6: 0.9891 - dense_3_acc_7: 0.9817 - dense_3_acc_8: 0.9643 - dense_3_acc_9: 0.9360 - dense_3_acc_10: 0.8923 - dense_3_acc_11: 0.8351 - dense_3_acc_12: 0.7672 - dense_3_acc_13: 0.7396 - dense_3_acc_14: 0.7530 - dense_3_acc_15: 0.8238 - dense_3_acc_16: 0.8980 - dense_3_acc_17: 0.9560 - dense_3_acc_18: 0.9655 - dense_3_acc_19: 0.9994\n",
      "Epoch 33/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.2068 - dense_3_loss: 0.0060 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9970 - dense_3_acc_5: 0.9939 - dense_3_acc_6: 0.9891 - dense_3_acc_7: 0.9824 - dense_3_acc_8: 0.9658 - dense_3_acc_9: 0.9363 - dense_3_acc_10: 0.8934 - dense_3_acc_11: 0.8326 - dense_3_acc_12: 0.7702 - dense_3_acc_13: 0.7458 - dense_3_acc_14: 0.7590 - dense_3_acc_15: 0.8257 - dense_3_acc_16: 0.8981 - dense_3_acc_17: 0.9566 - dense_3_acc_18: 0.9654 - dense_3_acc_19: 0.9996\n",
      "Epoch 34/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.0556 - dense_3_loss: 0.0059 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9971 - dense_3_acc_5: 0.9946 - dense_3_acc_6: 0.9894 - dense_3_acc_7: 0.9828 - dense_3_acc_8: 0.9650 - dense_3_acc_9: 0.9392 - dense_3_acc_10: 0.8967 - dense_3_acc_11: 0.8382 - dense_3_acc_12: 0.7751 - dense_3_acc_13: 0.7518 - dense_3_acc_14: 0.7682 - dense_3_acc_15: 0.8307 - dense_3_acc_16: 0.9028 - dense_3_acc_17: 0.9593 - dense_3_acc_18: 0.9681 - dense_3_acc_19: 0.9995\n",
      "Epoch 35/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 5.0106 - dense_3_loss: 0.0069 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9971 - dense_3_acc_5: 0.9942 - dense_3_acc_6: 0.9893 - dense_3_acc_7: 0.9818 - dense_3_acc_8: 0.9669 - dense_3_acc_9: 0.9393 - dense_3_acc_10: 0.8955 - dense_3_acc_11: 0.8407 - dense_3_acc_12: 0.7781 - dense_3_acc_13: 0.7515 - dense_3_acc_14: 0.7690 - dense_3_acc_15: 0.8329 - dense_3_acc_16: 0.9064 - dense_3_acc_17: 0.9578 - dense_3_acc_18: 0.9667 - dense_3_acc_19: 0.9992\n",
      "Epoch 36/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 4.8758 - dense_3_loss: 0.0056 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9973 - dense_3_acc_5: 0.9946 - dense_3_acc_6: 0.9895 - dense_3_acc_7: 0.9825 - dense_3_acc_8: 0.9644 - dense_3_acc_9: 0.9384 - dense_3_acc_10: 0.8989 - dense_3_acc_11: 0.8457 - dense_3_acc_12: 0.7812 - dense_3_acc_13: 0.7548 - dense_3_acc_14: 0.7784 - dense_3_acc_15: 0.8388 - dense_3_acc_16: 0.9096 - dense_3_acc_17: 0.9624 - dense_3_acc_18: 0.9687 - dense_3_acc_19: 0.9994\n",
      "Epoch 37/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 4.7709 - dense_3_loss: 0.0058 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9945 - dense_3_acc_6: 0.9899 - dense_3_acc_7: 0.9823 - dense_3_acc_8: 0.9667 - dense_3_acc_9: 0.9418 - dense_3_acc_10: 0.9019 - dense_3_acc_11: 0.8473 - dense_3_acc_12: 0.7883 - dense_3_acc_13: 0.7646 - dense_3_acc_14: 0.7809 - dense_3_acc_15: 0.8425 - dense_3_acc_16: 0.9134 - dense_3_acc_17: 0.9634 - dense_3_acc_18: 0.9698 - dense_3_acc_19: 0.9996\n",
      "Epoch 38/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 4.6737 - dense_3_loss: 0.0046 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9945 - dense_3_acc_6: 0.9900 - dense_3_acc_7: 0.9831 - dense_3_acc_8: 0.9677 - dense_3_acc_9: 0.9432 - dense_3_acc_10: 0.9046 - dense_3_acc_11: 0.8506 - dense_3_acc_12: 0.7937 - dense_3_acc_13: 0.7715 - dense_3_acc_14: 0.7873 - dense_3_acc_15: 0.8472 - dense_3_acc_16: 0.9137 - dense_3_acc_17: 0.9627 - dense_3_acc_18: 0.9711 - dense_3_acc_19: 0.9997\n",
      "Epoch 39/40\n",
      "10887/10887 [==============================] - 15s 1ms/step - loss: 4.5474 - dense_3_loss: 0.0046 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9972 - dense_3_acc_5: 0.9949 - dense_3_acc_6: 0.9899 - dense_3_acc_7: 0.9834 - dense_3_acc_8: 0.9690 - dense_3_acc_9: 0.9444 - dense_3_acc_10: 0.9076 - dense_3_acc_11: 0.8564 - dense_3_acc_12: 0.7987 - dense_3_acc_13: 0.7750 - dense_3_acc_14: 0.7951 - dense_3_acc_15: 0.8520 - dense_3_acc_16: 0.9172 - dense_3_acc_17: 0.9666 - dense_3_acc_18: 0.9728 - dense_3_acc_19: 0.9996\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10887/10887 [==============================] - 15s 1ms/step - loss: 4.4843 - dense_3_loss: 0.0042 - dense_3_acc: 0.9998 - dense_3_acc_1: 0.9997 - dense_3_acc_2: 0.9990 - dense_3_acc_3: 0.9983 - dense_3_acc_4: 0.9975 - dense_3_acc_5: 0.9949 - dense_3_acc_6: 0.9902 - dense_3_acc_7: 0.9821 - dense_3_acc_8: 0.9674 - dense_3_acc_9: 0.9454 - dense_3_acc_10: 0.9082 - dense_3_acc_11: 0.8585 - dense_3_acc_12: 0.8010 - dense_3_acc_13: 0.7778 - dense_3_acc_14: 0.7978 - dense_3_acc_15: 0.8541 - dense_3_acc_16: 0.9194 - dense_3_acc_17: 0.9672 - dense_3_acc_18: 0.9737 - dense_3_acc_19: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01143e8ef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, s0, c0], outputs, epochs=40, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai-Script to Roman-Script Translation\n",
    "* Task 4: Test your model on 5 examples of your choice including your name! (1 point)\n",
    "* Task 5: Show your visualization of attention scores on one of your example (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 66)\n"
     ]
    }
   ],
   "source": [
    "#task 4\n",
    "#fill your code here\n",
    "test = [\"ภาคิน\",\"ดีศรี\",\"พิสิษฐ์\",\"วจนสาระ\",\"ปวินท์\",\"เปี่ยมไทย\",\"นิธิวุฒิ\",\"วิไลนุช\",\"ชวิศ\",\"สกุลยืนยง\"]\n",
    "def prep_input(input_list):\n",
    "    X = []\n",
    "    for line in input_list:\n",
    "        temp=[]\n",
    "        for char in line:\n",
    "            temp.append(input_dict[char])\n",
    "        X.append(temp)\n",
    "    X = pad_sequences(X,maxlen=max_len)\n",
    "    x_padding = X\n",
    "    X= to_categorical(X,vocab_size)\n",
    "    X=X.reshape(len(input_list),max_len ,vocab_size)\n",
    "    \n",
    "    return X,x_padding\n",
    "EXAMPLES,x_padding = prep_input(test)\n",
    "print(EXAMPLES.shape)\n",
    "prediction = model.predict([EXAMPLES, s0, c0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 phakhin\n",
      "1 iiri\n",
      "2 phisit\n",
      "3 wataansara\n",
      "4 pawin\n",
      "5 iimathai\n",
      "6 ittwwwtthi\n",
      "7 wiraanut\n",
      "8 chawit\n",
      "9 uuuudanyuang\n"
     ]
    }
   ],
   "source": [
    "prediction = np.swapaxes(prediction,0,1)\n",
    "y_pred = prediction\n",
    "prediction = np.argmax(prediction, axis = -1)\n",
    "for j in range(len(prediction)):\n",
    "    output = \"\".join([rev_output[int(i)] if int(i)>1  else '' for i in prediction[j]])\n",
    "    print(j,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the attention map\n",
    "* If you need to install thai font: sudo apt install xfonts-thai\n",
    "* this is what your visualization might look like:\n",
    "<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/attn_viz_sample.png\"  style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 5\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family']='Loma' #you can change to other font that works for you\n",
    "#fill your code here\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(model, layer_name):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer\n",
    "\n",
    "att_layer = get_output_layer(model, \"attention_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 128), (10, 128))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0 = np.zeros((len(test), n_s))\n",
    "c0 = np.zeros((len(test), n_s))\n",
    "s0.shape, c0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 10, 20, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "percent = []\n",
    "for i in range(output_max_len):\n",
    "    output_fn = K.function(model.inputs, [att_layer.get_output_at(i)])\n",
    "    percent.append(output_fn([EXAMPLES, s0, c0]))\n",
    "percent = np.array(percent)\n",
    "percent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 20, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = percent[0]\n",
    "for i in percent[1:]:\n",
    "    tmp = np.concatenate((tmp,i),axis = -1)\n",
    "tmp = np.array(tmp)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "inp = [rev_input[int(i)] for i in x_padding[0]]\n",
    "out = [rev_output[int(i)] for i in prediction[0]]\n",
    "print(len(inp),len(out))\n",
    "df = pd.DataFrame(tmp[0][0],index = inp, columns = out)\n",
    "plt.rcParams['font.family']='Loma'\n",
    "ax = sns.heatmap(df[-7:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
